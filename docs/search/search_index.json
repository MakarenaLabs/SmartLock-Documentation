{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SmartLock Design Reference Introduction This documentation describes the architecture and design reference of the MakarenaLabs solution for a SmartLock based on 3D Face Recognition. Project Description Hardware & Software requirements How to run the examples KV260 Testbench Examples on how to use the APIs ( Kria and Zynq7000 ) Contacts For any question, please contact: Gordon Lau : gordon.lau@amd.com Robert Green : robert.green@amd.com Enrico Giordano : enrico.giordano@makarenalabs.com Guglielmo Zanni : guglielmo.zanni@makarenalabs.com Brixhilda Koci : brixhilda.koci@makarenalabs.com Matteo Castagnaro : matteo.castagnaro@makarenalabs.com","title":"Home"},{"location":"#smartlock-design-reference","text":"","title":"SmartLock Design Reference"},{"location":"#introduction","text":"This documentation describes the architecture and design reference of the MakarenaLabs solution for a SmartLock based on 3D Face Recognition. Project Description Hardware & Software requirements How to run the examples KV260 Testbench Examples on how to use the APIs ( Kria and Zynq7000 )","title":"Introduction"},{"location":"#contacts","text":"For any question, please contact: Gordon Lau : gordon.lau@amd.com Robert Green : robert.green@amd.com Enrico Giordano : enrico.giordano@makarenalabs.com Guglielmo Zanni : guglielmo.zanni@makarenalabs.com Brixhilda Koci : brixhilda.koci@makarenalabs.com Matteo Castagnaro : matteo.castagnaro@makarenalabs.com","title":"Contacts"},{"location":"hw-sw-requirements/","text":"Hardware & Software requirements The SmartLock solution uses different requirements, both software and hardware. In this section all these needs will be explained. Hardware requirements AMD/Xilinx Boards The SmartLock solution proposed here was built on two different AMD/Xilinx boards: Arty Z7-20 : The Arty Z7 is a ready-to-use development platform designed around the Zynq 7000 System-on-Chip from AMD. The Zynq 7000 architecture tightly integrates a dual-core, 650 MHz ARM Cortex-A9 processor with AMD 7 series Field Programmable Gate Array (FPGA) logic. For more information, please visit the official website ; Fig. 1 - Arty Z7-20 Kria KV260 : The KV260 is built for advanced vision application development without requiring complex hardware design knowledge. For more information, please visit the official website ; Fig. 2 - Kria KV260 2D and 3D Cameras The architecture of SmartLock solution is based on two type of USB camera, one to extract 3D face features and a 2D one to detect and recognize the person in front of it. To run the solution you need: 2D Camera : Logitech C505 or Logitech C310HD (you can use the USB camera of your choice, but these ones are strongly recommended); Logitech C310HD Logitech C505 3D Camera : PMD Flexx2_VGA from PMD Tech PMD Flexx2_VGA Miscellaneous Some other miscellaneous are required: MicroSD card : it must be greater than or equal to 16GB (32GB if you are going to run the demo into the Kria KV260 demo); MicroSD to USB adapter : to burn the image; USB Type-C/Type-A : to plug in the 3D camera into the board; Ethernet cable : it is needed for board internet connection; PC/Laptop : to start the demo; Router : to know the IP address of the board; Tripod : with clamping mechanism to hold 2D and 2D cameras ( this is an example); Optional (only for Kria KV260 board): Full-HD HDMI Monitor and HDMI cable : to show the GUI directly on the monitor plugged into the Kria KV260 board. In this case, you have to plug in the Full-HD HDMI monitor (1920x1080p) into the Kria KV260 HDMI port and work directly on the board's graphic interface; Keyboard and mouse kit : to run the commands into the Kria board; Software requirements There are some software that need to be installed into your PC in order to run the solution successfully. First of all, you need to have at least 22GB free on your disk in order to download the image to be burnt (it is more or less 19GB big). To burn the image, you need: 7Zip : to unarchive the downloaded image files; Balena Etcher : to burn the image. You can use one of your choice, but this one is strongly recommended; SSH Terminal : to connect to the board of your choice. If you are running the demo with a Windows PC, PuTTY or TeraTerm are recommended. Otherwise, if you are running on Linux, you can use the integrated terminal; Windows requirements If you choose to run the demo into the Kria KV260 board and you are running on a Windows PC, you need also the Xming Server in order to see the graphic user interface of the software. In this case, we show below how to set up Xming Server under PuTTY software. Please refer to the Xming Server website to know how to install it. First of all, open the PuTTY software and, in the list at the left in the main page, open Connection , click SSH \u2192 X11 , check the Enable X11 forwarding and in the X display location field write localhost:0 , as show in the image below. PuTTY X11 port forwarding","title":"Hardware & Software requirements"},{"location":"hw-sw-requirements/#hardware-software-requirements","text":"The SmartLock solution uses different requirements, both software and hardware. In this section all these needs will be explained.","title":"Hardware &amp; Software requirements"},{"location":"hw-sw-requirements/#hardware-requirements","text":"","title":"Hardware requirements"},{"location":"hw-sw-requirements/#amdxilinx-boards","text":"The SmartLock solution proposed here was built on two different AMD/Xilinx boards: Arty Z7-20 : The Arty Z7 is a ready-to-use development platform designed around the Zynq 7000 System-on-Chip from AMD. The Zynq 7000 architecture tightly integrates a dual-core, 650 MHz ARM Cortex-A9 processor with AMD 7 series Field Programmable Gate Array (FPGA) logic. For more information, please visit the official website ; Fig. 1 - Arty Z7-20 Kria KV260 : The KV260 is built for advanced vision application development without requiring complex hardware design knowledge. For more information, please visit the official website ; Fig. 2 - Kria KV260","title":"AMD/Xilinx Boards"},{"location":"hw-sw-requirements/#2d-and-3d-cameras","text":"The architecture of SmartLock solution is based on two type of USB camera, one to extract 3D face features and a 2D one to detect and recognize the person in front of it. To run the solution you need: 2D Camera : Logitech C505 or Logitech C310HD (you can use the USB camera of your choice, but these ones are strongly recommended); Logitech C310HD Logitech C505 3D Camera : PMD Flexx2_VGA from PMD Tech PMD Flexx2_VGA","title":"2D and 3D Cameras"},{"location":"hw-sw-requirements/#miscellaneous","text":"Some other miscellaneous are required: MicroSD card : it must be greater than or equal to 16GB (32GB if you are going to run the demo into the Kria KV260 demo); MicroSD to USB adapter : to burn the image; USB Type-C/Type-A : to plug in the 3D camera into the board; Ethernet cable : it is needed for board internet connection; PC/Laptop : to start the demo; Router : to know the IP address of the board; Tripod : with clamping mechanism to hold 2D and 2D cameras ( this is an example); Optional (only for Kria KV260 board): Full-HD HDMI Monitor and HDMI cable : to show the GUI directly on the monitor plugged into the Kria KV260 board. In this case, you have to plug in the Full-HD HDMI monitor (1920x1080p) into the Kria KV260 HDMI port and work directly on the board's graphic interface; Keyboard and mouse kit : to run the commands into the Kria board;","title":"Miscellaneous"},{"location":"hw-sw-requirements/#software-requirements","text":"There are some software that need to be installed into your PC in order to run the solution successfully. First of all, you need to have at least 22GB free on your disk in order to download the image to be burnt (it is more or less 19GB big). To burn the image, you need: 7Zip : to unarchive the downloaded image files; Balena Etcher : to burn the image. You can use one of your choice, but this one is strongly recommended; SSH Terminal : to connect to the board of your choice. If you are running the demo with a Windows PC, PuTTY or TeraTerm are recommended. Otherwise, if you are running on Linux, you can use the integrated terminal;","title":"Software requirements"},{"location":"hw-sw-requirements/#windows-requirements","text":"If you choose to run the demo into the Kria KV260 board and you are running on a Windows PC, you need also the Xming Server in order to see the graphic user interface of the software. In this case, we show below how to set up Xming Server under PuTTY software. Please refer to the Xming Server website to know how to install it. First of all, open the PuTTY software and, in the list at the left in the main page, open Connection , click SSH \u2192 X11 , check the Enable X11 forwarding and in the X display location field write localhost:0 , as show in the image below. PuTTY X11 port forwarding","title":"Windows requirements"},{"location":"project-description/","text":"SmartLock Design Reference Introduction The Machine Learning solution uses a combination of 2D and 3D information processing to identify a specific human who wishes to access a restricted area. Architectural Schematic The base architecture of the SmartLock is briefly described in following image: Fig. 1 - SmartLock Design Reference Architecture The system is based on a Machine Learning engine which uses AI models to extract features from both the 2D and 3D acquisition data. Both branches (2D and 3D) have their own dedicated AI model for feature extraction. The extracted information is then fused together in order to compute the authorization process. Below is a brief description of the two branches. 2D Branch This branch computes two different AI models in a cascade mode, first Face Detection and then Face Recognition. Face Detection is an AI model which analyzes an image from a video source. From this image are extracted, if present, the bounding boxes of the faces. The bounding boxes are a cropped version of the original image, containing only the faces, if present. Face Recognition is an AI model which receives the bounding box-cropped image of a face as input and returns a description of the face as output. The Face Recognition AI model is used to determine if the person in front of the SmartLock can be identified as someone who has been saved in the database as an authorized user. 2D APIs Introduction Those APIs are used to recognize the person who is standing in front of the lock. Those APIs are based on server-client modality which consists in asking a particular machine learning task. Those APIs are based on MuseBox. MuseBox is a Machine Learning framework developed by MakarenaLabs to infer AI models on FPGAs. The 2D branch of the SmartLock process performs two operations: Face Detection Face Recognition Face Detection consists in understanding whether a human face is present in a camera frame. If so, the frame is then cropped to the face. Face Recognition consists in \"describing\" the cropped face obtained before. The result of the description is used to label whether the person looking at the camera is present in the database of authorized faces. General Musebox Structure for AI/ML Tasks related to Face Analysis The MuseBox Server listens in localhost (127.0.0.1) at port 9696. In ZMQ socket binding, the Client will bind at this address to send data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\"/\"input\": <base64 image>/double array, \"publisherQueue\": <string> } Where: topic : is the name of the Machine Learning task image : is the OpenCV image in base64 format (string encoded in UTF8) publisherQueue : is the address where the Client expects to receive the response only_face : (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition. Face Detection It detects the faces in a scene up to 32\u00d732 pixel. Request from Client: \"topic\": \"FaceDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"boundingBox\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } boundingBox is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Face Recognition Given a face bounding box and a database of faces, it recognizes a face in a scene. Request from Client: \"topic\": \"FaceRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": { \"prediction\": <string> } } prediction corresponds to the name of the person Face Landmark Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": { \"prediction\": [196 float] } } prediction corresponds to (x,y) couples for face landmark points 3D Branch This branch is used to avoid efforts to spoof the 2D system. The function of this branch is to determine whether the 3D mapping of the person in front of the SmartLock is an human face. This rejects efforts to use images, photos, or other methods which can otherwise be accepted and recognized by the 2D system. 3D APIs Introduction As seen in the introduction, the architecture is divided in two branches. One of the two branches consists of the analysis and feature extraction of the 3D information of the system. The 3D information is extremely useful to understand the geometry and composition of a real 3D object. We exploit these properties to analyze whether a face lies in front of the camera. Why do we need to add this step? If we have only the 2D branch and our photo saved in the database, someone who wants to defeat the lock could simply show the camera a photo of an authorized individual (e.g., taken from a social network) and enter. Having a branch which analyzes that a real human face is in front of the camera prevents such spoofing techniques. To do this process we use a representation of the 3D information called Point Cloud. This is a data format which creates a cloud of points of the object that we want to represent. An example could be found in the below image: Fig. 1 - Example of Point Cloud In our setup we need 3 types of APIs to manipulate the Point Cloud obtained by the depth sensor. Those APIs are based on the BSD-licensed Point Cloud Library . The first type of API is a point cloud downsampler, which reduces the computational effort, of subsequent steps. The downsampler reduces the number of points in the cloud while maintaining its geometrical properties (so a face remains a face). With less points it is easier to manipulate the Point Cloud and extract information and features out of it. We will refer to it as Downsampler . The second type of API which we need is something which can extract different components out of a Point Cloud. Extracting this subgroup s of points is a process called segmentation and is used to isolate different object in a Point Cloud. This process is carried out by grouping points that are close one to each other and discard the ones that are far away, given a certain distance threshold. We will use the Segmenter to isolate the face of the person from the background points obtained by the acquisition of the 3D sensor. We will suppose in our system the face to be in front of the camera. We refer to the object which does this operation as Segmenter . The third type of API is one which extracts features out of the Point Cloud. We refer to this as Feature Extractor 3D . This type of API is used to compute a vector of descriptors of our Point Cloud. We are going to compare these vectors between Point Clouds to see how much two Point Clouds are similar. In our case, we will categorize if the Point Cloud is sufficiently similar to a human face before proceeding with the unlocking process. The basic algorithm for the 3D branch can be then summarized in these three steps: Check if the Point Cloud has too many points, and if so, downsample using the Downsampler . Isolate the face from point cloud, using the Segmenter . Extract the features out of the Point Cloud and check how close they resemble a face using the 3D Feature Extractor . Class name: Downsampler This object is used to voxelize the point cloud. The voxelization process results in a less dense 3D representation, where the points are grouped into voxels. The voxels are computed by group of points that are close in space. In our intent we will represent more points with a single voxel, thus resulting in an actual downsampling operation for the Point Cloud. The voxels are computed by grouping points in \"leaves\" (can be seen as cubes) with modifiable X-Y-Z axis value to enlarge/stretch them. downsampler(float leaf_size_x, float leaf_size_y, float leaf_size_z); Constructor of the class Function params : leaf_size_x : Size on the X axis of the voxel leaf_size_y : Size on the Y axis of the voxel leaf_size_z : Size on the Z axis of the voxel pcl::PointCloud<pcl::PointXYZ> compute_downsampling(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud); Computes the voxelization on the point cloud and returns the downsampled cloud. Function params : cloud : pointer to the point cloud we want to downsample. void set_leaf_size_x(float x); Set the value of the X axis of the voxel. Overwrites what has been set during the construction of the object. Function params : x : pointer to the point cloud we want to downsample. void set_leaf_size_y(float y); Set the value of the Y axis of the voxel. Overwrites what has been set during the construction of the object. Function params : y : pointer to the point cloud we want to downsample. void set_leaf_size_z(float z); Set the value of the Z axis of the voxel. Overwrites what has been set during the construction of the object. Function params : z : pointer to the point cloud we want to downsample. Class name: Segmenter This class implements a Euclidean Segmenter. This Segmenter divides the Point Cloud chosen into subgroups. The metric chosen to create the subgroups is the Euclidean distance. Out of this subgroups we will return just the biggest. The logic behind this choice is that the face is placed in front of the camera, and thus most of the points represent the face itself. If other points are present in the background they are not part of the face and can be discarded. segmenter(); Constructor of the class pcl::PointCloud<pcl::PointXYZ> clustering(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud); This methods accepts in input a Point Cloud, segments it and return the biggest of the subgroup created. Function params : cloud : The Point Cloud to segment. On the Zynq7000, the Python version of this API is available, and not the C++ implementation. Reported below is the signature, as the name of the functions and their application do not diverge with respect to what has been already explained. To import the API just tap: import musebox_pcl_bindings_segmenter as ec And then, to use it tap: ec.euclidean_clustering(point_cloud) Functions Params : point_cloud : A Nx3 shaped numpy array which represents our Point Cloud Class name: Feature Extractor 3D This object is used to compute the descriptors of the point cloud. The descriptors are dependent on the geometry of the point cloud itself. In our application this results in checking the resemblance of this group of points with a face. feature_extractor_3d(metrics feature_comparison_metric); Constructor of the class Function params : feature_comparison_metric : How the loss of the features extracted is computed. Only the L2 loss is supported. std::tuple< float, pcl::PointCloud<pcl::VFHSignature308> > get_features(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud1, pcl::PointCloud<pcl::PointXYZ>::Ptr cloud2); Constructor of the class Function params : cloud1 : First Point Cloud in input. In this case, it represents the features of the face in front of the 3D camera; cloud2 : Cloud to be compared with cloud1. On the Zynq7000, the Python version of this API is available, and not the C++ implementation. Reported below is the signature, as the name of the functions and their application do not diverge with respect to what has been already explained. To import the API just tap: import musebox_pcl_bindings_feature_extractor as fe3d And then, to use it tap: fe3d.feature_extractor_3d(segmented_point_cloud, face_point_cloud) Functions Params : segmented_point_cloud : A Nx3 shaped numpy array which represents our Point Cloud (obtained by the segmenter) face_point_cloud : A Nx3 shaped numpy array which represents our face Point Cloud","title":"Project Description"},{"location":"project-description/#smartlock-design-reference","text":"","title":"SmartLock Design Reference"},{"location":"project-description/#introduction","text":"The Machine Learning solution uses a combination of 2D and 3D information processing to identify a specific human who wishes to access a restricted area.","title":"Introduction"},{"location":"project-description/#architectural-schematic","text":"The base architecture of the SmartLock is briefly described in following image: Fig. 1 - SmartLock Design Reference Architecture The system is based on a Machine Learning engine which uses AI models to extract features from both the 2D and 3D acquisition data. Both branches (2D and 3D) have their own dedicated AI model for feature extraction. The extracted information is then fused together in order to compute the authorization process. Below is a brief description of the two branches.","title":"Architectural Schematic"},{"location":"project-description/#2d-branch","text":"This branch computes two different AI models in a cascade mode, first Face Detection and then Face Recognition. Face Detection is an AI model which analyzes an image from a video source. From this image are extracted, if present, the bounding boxes of the faces. The bounding boxes are a cropped version of the original image, containing only the faces, if present. Face Recognition is an AI model which receives the bounding box-cropped image of a face as input and returns a description of the face as output. The Face Recognition AI model is used to determine if the person in front of the SmartLock can be identified as someone who has been saved in the database as an authorized user.","title":"2D Branch"},{"location":"project-description/#2d-apis","text":"","title":"2D APIs"},{"location":"project-description/#introduction_1","text":"Those APIs are used to recognize the person who is standing in front of the lock. Those APIs are based on server-client modality which consists in asking a particular machine learning task. Those APIs are based on MuseBox. MuseBox is a Machine Learning framework developed by MakarenaLabs to infer AI models on FPGAs. The 2D branch of the SmartLock process performs two operations: Face Detection Face Recognition Face Detection consists in understanding whether a human face is present in a camera frame. If so, the frame is then cropped to the face. Face Recognition consists in \"describing\" the cropped face obtained before. The result of the description is used to label whether the person looking at the camera is present in the database of authorized faces.","title":"Introduction"},{"location":"project-description/#general-musebox-structure-for-aiml-tasks-related-to-face-analysis","text":"The MuseBox Server listens in localhost (127.0.0.1) at port 9696. In ZMQ socket binding, the Client will bind at this address to send data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\"/\"input\": <base64 image>/double array, \"publisherQueue\": <string> } Where: topic : is the name of the Machine Learning task image : is the OpenCV image in base64 format (string encoded in UTF8) publisherQueue : is the address where the Client expects to receive the response only_face : (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition.","title":"General Musebox Structure for AI/ML Tasks related to Face Analysis"},{"location":"project-description/#face-detection","text":"It detects the faces in a scene up to 32\u00d732 pixel. Request from Client: \"topic\": \"FaceDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"boundingBox\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } boundingBox is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Face Detection"},{"location":"project-description/#face-recognition","text":"Given a face bounding box and a database of faces, it recognizes a face in a scene. Request from Client: \"topic\": \"FaceRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": { \"prediction\": <string> } } prediction corresponds to the name of the person","title":"Face Recognition"},{"location":"project-description/#face-landmark","text":"Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": { \"prediction\": [196 float] } } prediction corresponds to (x,y) couples for face landmark points","title":"Face Landmark"},{"location":"project-description/#3d-branch","text":"This branch is used to avoid efforts to spoof the 2D system. The function of this branch is to determine whether the 3D mapping of the person in front of the SmartLock is an human face. This rejects efforts to use images, photos, or other methods which can otherwise be accepted and recognized by the 2D system.","title":"3D Branch"},{"location":"project-description/#3d-apis","text":"","title":"3D APIs"},{"location":"project-description/#introduction_2","text":"As seen in the introduction, the architecture is divided in two branches. One of the two branches consists of the analysis and feature extraction of the 3D information of the system. The 3D information is extremely useful to understand the geometry and composition of a real 3D object. We exploit these properties to analyze whether a face lies in front of the camera. Why do we need to add this step? If we have only the 2D branch and our photo saved in the database, someone who wants to defeat the lock could simply show the camera a photo of an authorized individual (e.g., taken from a social network) and enter. Having a branch which analyzes that a real human face is in front of the camera prevents such spoofing techniques. To do this process we use a representation of the 3D information called Point Cloud. This is a data format which creates a cloud of points of the object that we want to represent. An example could be found in the below image: Fig. 1 - Example of Point Cloud In our setup we need 3 types of APIs to manipulate the Point Cloud obtained by the depth sensor. Those APIs are based on the BSD-licensed Point Cloud Library . The first type of API is a point cloud downsampler, which reduces the computational effort, of subsequent steps. The downsampler reduces the number of points in the cloud while maintaining its geometrical properties (so a face remains a face). With less points it is easier to manipulate the Point Cloud and extract information and features out of it. We will refer to it as Downsampler . The second type of API which we need is something which can extract different components out of a Point Cloud. Extracting this subgroup s of points is a process called segmentation and is used to isolate different object in a Point Cloud. This process is carried out by grouping points that are close one to each other and discard the ones that are far away, given a certain distance threshold. We will use the Segmenter to isolate the face of the person from the background points obtained by the acquisition of the 3D sensor. We will suppose in our system the face to be in front of the camera. We refer to the object which does this operation as Segmenter . The third type of API is one which extracts features out of the Point Cloud. We refer to this as Feature Extractor 3D . This type of API is used to compute a vector of descriptors of our Point Cloud. We are going to compare these vectors between Point Clouds to see how much two Point Clouds are similar. In our case, we will categorize if the Point Cloud is sufficiently similar to a human face before proceeding with the unlocking process. The basic algorithm for the 3D branch can be then summarized in these three steps: Check if the Point Cloud has too many points, and if so, downsample using the Downsampler . Isolate the face from point cloud, using the Segmenter . Extract the features out of the Point Cloud and check how close they resemble a face using the 3D Feature Extractor .","title":"Introduction"},{"location":"project-description/#class-name-downsampler","text":"This object is used to voxelize the point cloud. The voxelization process results in a less dense 3D representation, where the points are grouped into voxels. The voxels are computed by group of points that are close in space. In our intent we will represent more points with a single voxel, thus resulting in an actual downsampling operation for the Point Cloud. The voxels are computed by grouping points in \"leaves\" (can be seen as cubes) with modifiable X-Y-Z axis value to enlarge/stretch them. downsampler(float leaf_size_x, float leaf_size_y, float leaf_size_z); Constructor of the class Function params : leaf_size_x : Size on the X axis of the voxel leaf_size_y : Size on the Y axis of the voxel leaf_size_z : Size on the Z axis of the voxel pcl::PointCloud<pcl::PointXYZ> compute_downsampling(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud); Computes the voxelization on the point cloud and returns the downsampled cloud. Function params : cloud : pointer to the point cloud we want to downsample. void set_leaf_size_x(float x); Set the value of the X axis of the voxel. Overwrites what has been set during the construction of the object. Function params : x : pointer to the point cloud we want to downsample. void set_leaf_size_y(float y); Set the value of the Y axis of the voxel. Overwrites what has been set during the construction of the object. Function params : y : pointer to the point cloud we want to downsample. void set_leaf_size_z(float z); Set the value of the Z axis of the voxel. Overwrites what has been set during the construction of the object. Function params : z : pointer to the point cloud we want to downsample.","title":"Class name: Downsampler"},{"location":"project-description/#class-name-segmenter","text":"This class implements a Euclidean Segmenter. This Segmenter divides the Point Cloud chosen into subgroups. The metric chosen to create the subgroups is the Euclidean distance. Out of this subgroups we will return just the biggest. The logic behind this choice is that the face is placed in front of the camera, and thus most of the points represent the face itself. If other points are present in the background they are not part of the face and can be discarded. segmenter(); Constructor of the class pcl::PointCloud<pcl::PointXYZ> clustering(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud); This methods accepts in input a Point Cloud, segments it and return the biggest of the subgroup created. Function params : cloud : The Point Cloud to segment. On the Zynq7000, the Python version of this API is available, and not the C++ implementation. Reported below is the signature, as the name of the functions and their application do not diverge with respect to what has been already explained. To import the API just tap: import musebox_pcl_bindings_segmenter as ec And then, to use it tap: ec.euclidean_clustering(point_cloud) Functions Params : point_cloud : A Nx3 shaped numpy array which represents our Point Cloud","title":"Class name: Segmenter"},{"location":"project-description/#class-name-feature-extractor-3d","text":"This object is used to compute the descriptors of the point cloud. The descriptors are dependent on the geometry of the point cloud itself. In our application this results in checking the resemblance of this group of points with a face. feature_extractor_3d(metrics feature_comparison_metric); Constructor of the class Function params : feature_comparison_metric : How the loss of the features extracted is computed. Only the L2 loss is supported. std::tuple< float, pcl::PointCloud<pcl::VFHSignature308> > get_features(pcl::PointCloud<pcl::PointXYZ>::Ptr cloud1, pcl::PointCloud<pcl::PointXYZ>::Ptr cloud2); Constructor of the class Function params : cloud1 : First Point Cloud in input. In this case, it represents the features of the face in front of the 3D camera; cloud2 : Cloud to be compared with cloud1. On the Zynq7000, the Python version of this API is available, and not the C++ implementation. Reported below is the signature, as the name of the functions and their application do not diverge with respect to what has been already explained. To import the API just tap: import musebox_pcl_bindings_feature_extractor as fe3d And then, to use it tap: fe3d.feature_extractor_3d(segmented_point_cloud, face_point_cloud) Functions Params : segmented_point_cloud : A Nx3 shaped numpy array which represents our Point Cloud (obtained by the segmenter) face_point_cloud : A Nx3 shaped numpy array which represents our face Point Cloud","title":"Class name: Feature Extractor 3D"},{"location":"code-analysis/arty-z7/","text":"SmartLock example on Zynq7000 Introduction We will now paste the code of the example application of the SmartLock running on a Zynq7000 (in our case an Arty-7Z020 board) and explain how it works piece by piece. #!/usr/bin/env python # coding: utf-8 import numpy as np from driver import io_shape_dict from driver_base import FINNExampleOverlay import time import cv2 import zmq import json import base64 import imageio import io import os import musebox_pcl_bindings_feature_extractor as fe3d import musebox_pcl_bindings_segmenter as ec import argparse from yunet import YuNet mean = [0.6071, 0.4609, 0.3944] std = [0.2457, 0.2175, 0.2129] database_dict_list = [] # Load the cascade face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') bsize = 1 bitfile = \"/home/xilinx/jupyter_notebooks/smart-lock/bitfile/smartlock.bit\" weights = \"/home/xilinx/jupyter_notebooks/smart-lock/driver/runtime_weights/\" platform = \"zynq-iodma\" driver = FINNExampleOverlay( bitfile_name=bitfile, platform=platform, io_shape_dict=io_shape_dict, batch_size=bsize, runtime_weight_dir=weights, ) def run(image_input): global driver, mean, std image_input = image_input.astype(np.float32) image_input[:,:,0] /= 255 image_input[:,:,1] /= 255 image_input[:,:,2] /= 255 image_input = cv2.resize(image_input, (140, 140)) image_input[:,:,0] -= mean[0] image_input[:,:,1] -= mean[1] image_input[:,:,2] -= mean[2] image_input[:,:,0] /= std[0] image_input[:,:,1] /= std[1] image_input[:,:,2] /= std[2] image_input *= 255 ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape) driver.copy_input_data_to_device(ibuf_normal) driver.execute_on_buffers() obuf_normal = np.empty_like(driver.obuf_packed_device[0]) driver.copy_output_data_from_device(obuf_normal) obuf_folded = driver.unpack_output(obuf_normal) final_output = driver.unfold_output(obuf_folded) return final_output # ip 192.168.188.107 def init_sender(ip=\"tcp://*:5557\"): global context, socket_pub ############## SEND ############# socket_pub = context.socket(zmq.PUB) socket_pub.bind(\"tcp://*:5557\") context = zmq.Context() socket = None socket_pub = None def init_listener(ip=\"tcp://192.168.188.60:5556\"): global context, socket # CREATE COMMUNICATION LISTENER # context = zmq.Context() ############## RECEIVE ############# socket = context.socket(zmq.SUB) socket.connect(ip) socket.setsockopt_string(zmq.SUBSCRIBE, str('')) def get_data(): global context, socket message = None i = 0 while i < 1: try: zmq_response = socket.recv() # print(\"has received\") message = json.loads(zmq_response) i = i + 1 except Exception as e: print(\"Exception: \", e) return message def send_trigger(trigger: str): global socket_pub message = {\"trigger\": trigger} socket_pub.send_string(json.dumps(message)) def init_database(folder: str): global database_dict_list for filename in os.listdir(folder): img = cv2.imread(os.path.join(folder, filename)) features = run(img) if img is not None: database_dict_list.append({\"features\": features, \"label\": filename}) def init(ip_address: str): ip_address_ = \"tcp://\" + ip_address + \":5556\" init_listener(ip_address_) init_sender() init_database(\"database\") def main_sl(ip_address: str, camera_index: int): init(ip_address) model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) pcl_similarity = 2500 fr_similarity = 700 aspect_ratio = (320/320, 320/320) print(\"Start Smart Lock Routine\") while True: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue sim_ = 99999 person_ = \"\" try: image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] features = run(image) for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\") send_trigger(\"continue\") except: send_trigger(\"continue\") pass if __name__ == \"__main__\": # Create the parser parser = argparse.ArgumentParser()# Add an argument parser.add_argument('--ip', type=str, required=True)# Parse the argument parser.add_argument('--camera_index', type=int, required=True)# Parse the argument args = parser.parse_args() main_sl(args.ip, args.camera_index) We now dig function per function and explain its functionality. Run Face Recognition def run(image_input): global driver, mean, std image_input = image_input.astype(np.float32) image_input[:,:,0] /= 255 image_input[:,:,1] /= 255 image_input[:,:,2] /= 255 image_input = cv2.resize(image_input, (140, 140)) image_input[:,:,0] -= mean[0] image_input[:,:,1] -= mean[1] image_input[:,:,2] -= mean[2] image_input[:,:,0] /= std[0] image_input[:,:,1] /= std[1] image_input[:,:,2] /= std[2] image_input *= 255 ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape) driver.copy_input_data_to_device(ibuf_normal) driver.execute_on_buffers() obuf_normal = np.empty_like(driver.obuf_packed_device[0]) driver.copy_output_data_from_device(obuf_normal) obuf_folded = driver.unpack_output(obuf_normal) final_output = driver.unfold_output(obuf_folded) return final_output This function takes in input a bounding box processed using the python bindings of OpenCV and return the features that describe it. The feature extraction is performed using our custom Face Recognition model. The image normalization is mandatory before inferring the model, but is taken care from the function. TCP connection with an external workstation def init_sender(ip=\"tcp://*:5557\"): global context, socket_pub ############## SEND ############# socket_pub = context.socket(zmq.PUB) socket_pub.bind(\"tcp://*:5557\") context = zmq.Context() socket = None socket_pub = None def init_listener(ip=\"tcp://192.168.188.60:5556\"): global context, socket # CREATE COMMUNICATION LISTENER # context = zmq.Context() ############## RECEIVE ############# socket = context.socket(zmq.SUB) socket.connect(ip) socket.setsockopt_string(zmq.SUBSCRIBE, str('')) def get_data(): global context, socket message = None i = 0 while i < 1: try: zmq_response = socket.recv() # print(\"has received\") message = json.loads(zmq_response) i = i + 1 except Exception as e: print(\"Exception: \", e) return message def send_trigger(trigger: str): global socket_pub message = {\"trigger\": trigger} socket_pub.send_string(json.dumps(message)) This group of functions creates a publisher and a subscriber, based on the ZMQ library and the TCP protocol, used to create a double communication channel between the host pc and the Zynq7000. The functions init_listener and init_sender initialize the subscriber and the publisher respectively. Both take as argument the respective IP address publisher/subscriber (or rather the host PC and the Zynq7000). For the publisher we will adopt the technique of broadcasting the messages to every subscriber listening to a specific port, so no particular IP address would be required to use this function. send_trigger is used to sync the read/write mechanism between the host pc and the Zynq7000. The host PC listens to the Arty, and sends a message every time a trigger is sent from the Zynq7000 using this function. Load the photos to be recognized def init_database(folder: str): global database_dict_list for filename in os.listdir(folder): img = cv2.imread(os.path.join(folder, filename)) features = run(img) if img is not None: database_dict_list.append({\"features\": features, \"label\": filename}) This function loads the photos to be recognized and extracts a vocabulary of features and a label out of it. The features will be used in the Face Recognition process, and the label is used to assign a name if a person is recognized in the process. Main Loop def main_sl(ip_address: str, camera_index: int): init(ip_address) model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) pcl_similarity = 2500 fr_similarity = 700 aspect_ratio = (320/320, 320/320) print(\"Start Smart Lock Routine\") while True: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue sim_ = 99999 person_ = \"\" try: image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] features = run(image) for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\") send_trigger(\"continue\") except: send_trigger(\"continue\") pass In the main loop we put together all the functions to create our desired flow. First the publisher, subscriber and vocabulary of people are initialized through the function init . init(ip_address) Secondly, we load the model of Face Detection in the variable model . model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) We initialize the camera using the variable cap . This is our object which grabs the 2D frames. try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) We set our similarity threshold values for Face Recognition ( fr_similarity ) and 3D Person Detection ( pcl_similarity ). The lower those values are the more robust the system is, but with less probability to give an output. pcl_similarity = 2500 fr_similarity = 700 We receive the messages from the publisher of the 3D acquisition and grab simultaneously a frame from the 2D camera: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue We check that there is actually a face in front of the camera. result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue If there is a person, we start the 2D branch, by firstly detecting and cropping a face. image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] We perform Face Recognition: features = run(image) And then we check if the person is present into the database, and if so we assign a label to the most likely similar person, otherwise we restart the loop: for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\")","title":"Arty Z7-20"},{"location":"code-analysis/arty-z7/#smartlock-example-on-zynq7000","text":"","title":"SmartLock example on Zynq7000"},{"location":"code-analysis/arty-z7/#introduction","text":"We will now paste the code of the example application of the SmartLock running on a Zynq7000 (in our case an Arty-7Z020 board) and explain how it works piece by piece. #!/usr/bin/env python # coding: utf-8 import numpy as np from driver import io_shape_dict from driver_base import FINNExampleOverlay import time import cv2 import zmq import json import base64 import imageio import io import os import musebox_pcl_bindings_feature_extractor as fe3d import musebox_pcl_bindings_segmenter as ec import argparse from yunet import YuNet mean = [0.6071, 0.4609, 0.3944] std = [0.2457, 0.2175, 0.2129] database_dict_list = [] # Load the cascade face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') bsize = 1 bitfile = \"/home/xilinx/jupyter_notebooks/smart-lock/bitfile/smartlock.bit\" weights = \"/home/xilinx/jupyter_notebooks/smart-lock/driver/runtime_weights/\" platform = \"zynq-iodma\" driver = FINNExampleOverlay( bitfile_name=bitfile, platform=platform, io_shape_dict=io_shape_dict, batch_size=bsize, runtime_weight_dir=weights, ) def run(image_input): global driver, mean, std image_input = image_input.astype(np.float32) image_input[:,:,0] /= 255 image_input[:,:,1] /= 255 image_input[:,:,2] /= 255 image_input = cv2.resize(image_input, (140, 140)) image_input[:,:,0] -= mean[0] image_input[:,:,1] -= mean[1] image_input[:,:,2] -= mean[2] image_input[:,:,0] /= std[0] image_input[:,:,1] /= std[1] image_input[:,:,2] /= std[2] image_input *= 255 ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape) driver.copy_input_data_to_device(ibuf_normal) driver.execute_on_buffers() obuf_normal = np.empty_like(driver.obuf_packed_device[0]) driver.copy_output_data_from_device(obuf_normal) obuf_folded = driver.unpack_output(obuf_normal) final_output = driver.unfold_output(obuf_folded) return final_output # ip 192.168.188.107 def init_sender(ip=\"tcp://*:5557\"): global context, socket_pub ############## SEND ############# socket_pub = context.socket(zmq.PUB) socket_pub.bind(\"tcp://*:5557\") context = zmq.Context() socket = None socket_pub = None def init_listener(ip=\"tcp://192.168.188.60:5556\"): global context, socket # CREATE COMMUNICATION LISTENER # context = zmq.Context() ############## RECEIVE ############# socket = context.socket(zmq.SUB) socket.connect(ip) socket.setsockopt_string(zmq.SUBSCRIBE, str('')) def get_data(): global context, socket message = None i = 0 while i < 1: try: zmq_response = socket.recv() # print(\"has received\") message = json.loads(zmq_response) i = i + 1 except Exception as e: print(\"Exception: \", e) return message def send_trigger(trigger: str): global socket_pub message = {\"trigger\": trigger} socket_pub.send_string(json.dumps(message)) def init_database(folder: str): global database_dict_list for filename in os.listdir(folder): img = cv2.imread(os.path.join(folder, filename)) features = run(img) if img is not None: database_dict_list.append({\"features\": features, \"label\": filename}) def init(ip_address: str): ip_address_ = \"tcp://\" + ip_address + \":5556\" init_listener(ip_address_) init_sender() init_database(\"database\") def main_sl(ip_address: str, camera_index: int): init(ip_address) model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) pcl_similarity = 2500 fr_similarity = 700 aspect_ratio = (320/320, 320/320) print(\"Start Smart Lock Routine\") while True: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue sim_ = 99999 person_ = \"\" try: image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] features = run(image) for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\") send_trigger(\"continue\") except: send_trigger(\"continue\") pass if __name__ == \"__main__\": # Create the parser parser = argparse.ArgumentParser()# Add an argument parser.add_argument('--ip', type=str, required=True)# Parse the argument parser.add_argument('--camera_index', type=int, required=True)# Parse the argument args = parser.parse_args() main_sl(args.ip, args.camera_index) We now dig function per function and explain its functionality.","title":"Introduction"},{"location":"code-analysis/arty-z7/#run-face-recognition","text":"def run(image_input): global driver, mean, std image_input = image_input.astype(np.float32) image_input[:,:,0] /= 255 image_input[:,:,1] /= 255 image_input[:,:,2] /= 255 image_input = cv2.resize(image_input, (140, 140)) image_input[:,:,0] -= mean[0] image_input[:,:,1] -= mean[1] image_input[:,:,2] -= mean[2] image_input[:,:,0] /= std[0] image_input[:,:,1] /= std[1] image_input[:,:,2] /= std[2] image_input *= 255 ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape) driver.copy_input_data_to_device(ibuf_normal) driver.execute_on_buffers() obuf_normal = np.empty_like(driver.obuf_packed_device[0]) driver.copy_output_data_from_device(obuf_normal) obuf_folded = driver.unpack_output(obuf_normal) final_output = driver.unfold_output(obuf_folded) return final_output This function takes in input a bounding box processed using the python bindings of OpenCV and return the features that describe it. The feature extraction is performed using our custom Face Recognition model. The image normalization is mandatory before inferring the model, but is taken care from the function.","title":"Run Face Recognition"},{"location":"code-analysis/arty-z7/#tcp-connection-with-an-external-workstation","text":"def init_sender(ip=\"tcp://*:5557\"): global context, socket_pub ############## SEND ############# socket_pub = context.socket(zmq.PUB) socket_pub.bind(\"tcp://*:5557\") context = zmq.Context() socket = None socket_pub = None def init_listener(ip=\"tcp://192.168.188.60:5556\"): global context, socket # CREATE COMMUNICATION LISTENER # context = zmq.Context() ############## RECEIVE ############# socket = context.socket(zmq.SUB) socket.connect(ip) socket.setsockopt_string(zmq.SUBSCRIBE, str('')) def get_data(): global context, socket message = None i = 0 while i < 1: try: zmq_response = socket.recv() # print(\"has received\") message = json.loads(zmq_response) i = i + 1 except Exception as e: print(\"Exception: \", e) return message def send_trigger(trigger: str): global socket_pub message = {\"trigger\": trigger} socket_pub.send_string(json.dumps(message)) This group of functions creates a publisher and a subscriber, based on the ZMQ library and the TCP protocol, used to create a double communication channel between the host pc and the Zynq7000. The functions init_listener and init_sender initialize the subscriber and the publisher respectively. Both take as argument the respective IP address publisher/subscriber (or rather the host PC and the Zynq7000). For the publisher we will adopt the technique of broadcasting the messages to every subscriber listening to a specific port, so no particular IP address would be required to use this function. send_trigger is used to sync the read/write mechanism between the host pc and the Zynq7000. The host PC listens to the Arty, and sends a message every time a trigger is sent from the Zynq7000 using this function.","title":"TCP connection with an external workstation"},{"location":"code-analysis/arty-z7/#load-the-photos-to-be-recognized","text":"def init_database(folder: str): global database_dict_list for filename in os.listdir(folder): img = cv2.imread(os.path.join(folder, filename)) features = run(img) if img is not None: database_dict_list.append({\"features\": features, \"label\": filename}) This function loads the photos to be recognized and extracts a vocabulary of features and a label out of it. The features will be used in the Face Recognition process, and the label is used to assign a name if a person is recognized in the process.","title":"Load the photos to be recognized"},{"location":"code-analysis/arty-z7/#main-loop","text":"def main_sl(ip_address: str, camera_index: int): init(ip_address) model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) pcl_similarity = 2500 fr_similarity = 700 aspect_ratio = (320/320, 320/320) print(\"Start Smart Lock Routine\") while True: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue sim_ = 99999 person_ = \"\" try: image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] features = run(image) for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\") send_trigger(\"continue\") except: send_trigger(\"continue\") pass In the main loop we put together all the functions to create our desired flow. First the publisher, subscriber and vocabulary of people are initialized through the function init . init(ip_address) Secondly, we load the model of Face Detection in the variable model . model = YuNet(modelPath=\"face_detection_yunet_2022mar.onnx\", inputSize=[320, 320], confThreshold=0.9, nmsThreshold=0.3, topK=5000, ) We initialize the camera using the variable cap . This is our object which grabs the 2D frames. try: cap.release() except: pass cap = cv2.VideoCapture(camera_index) cap.set(cv2.CAP_PROP_BUFFERSIZE, 1) We set our similarity threshold values for Face Recognition ( fr_similarity ) and 3D Person Detection ( pcl_similarity ). The lower those values are the more robust the system is, but with less probability to give an output. pcl_similarity = 2500 fr_similarity = 700 We receive the messages from the publisher of the 3D acquisition and grab simultaneously a frame from the 2D camera: message_recv = get_data() _, img = cap.read() if img is None: print(\"I can't read from the camera!\") continue We check that there is actually a face in front of the camera. result_segmenter = ec.euclidean_clustering(message_recv[\"point_cloud_camera\"]) res = fe3d.feature_extractor_3d(result_segmenter, message_recv[\"point_cloud_database\"]) if res > pcl_similarity: print(\"NOT A PERSON!\") print(res) send_trigger(\"continue\") continue If there is a person, we start the 2D branch, by firstly detecting and cropping a face. image = img.copy() image = cv2.resize(image, (320, 320)) img = cv2.resize(img, (320, 320)) model.setInputSize([320, 320]) result = model.infer(img) output = None for det in result: bbox = det[0:4].astype(np.int32) output = bbox break image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])] We perform Face Recognition: features = run(image) And then we check if the person is present into the database, and if so we assign a label to the most likely similar person, otherwise we restart the loop: for elem in database_dict_list: sim_face = np.linalg.norm(features - elem[\"features\"]) if sim_face < fr_similarity: if sim_face < sim_: sim_ = sim_face person_ = elem[\"label\"] if sim_ != 99999: sim_ = 99999 print(\"person is: \", person_) else: print(\"UNKOWN\")","title":"Main Loop"},{"location":"code-analysis/kria-kv260/","text":"Kria KV260 Example Introduction We will now paste the code of the example application of the SmartLock running on a Kria SOM (in our case a KV260 board) and explain how it works piece by piece. The project is composed of 4 main classes: easy_zmq , which implements the communication between the software and MuseBox Server using ZMQ library; easy_ncurses , which is used to manage the information showed in the console window; cv_show , which exposes some functions to draw a graphic interface; camera2d , which handles 2D camera functions; We use ncurses library to have a better output displayed in the terminal window. Then, we now dig step by step and explain all the software functionalities. Click to expand the entire main.cpp content // #include <royale.hpp> #include \"../include/utils.hpp\" #include \"../include/camera2d.hpp\" #include \"../include/cv_show.hpp\" #include \"../include/3d_face_points.hpp\" #include \"../include/easy_zmq.hpp\" #include \"../include/easy_ncurses.hpp\" #include \"../include/point_cloud/feature_extractor_3d.hpp\" #include \"../include/point_cloud/downsampler.hpp\" #include \"../include/point_cloud/segmenter.hpp\" #include \"../include/libroyale3d/LibRoyale3D.hpp\" pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_person(new pcl::PointCloud<pcl::PointXYZ>()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_clustered(new pcl::PointCloud<pcl::PointXYZ>()); int main(int argc, char *argv[]) { if (argc < 3) { std::cerr << \" need as input the number of the camera and similarity\" << std::endl; } unsigned int index = std::atoi(argv[1]); float similarity_ = std::atof(argv[2]); int listening_port = 5556; bool unlocked = false; int button_pressed = -1; bool debug = false; bool show = false; bool ncur_active = true; bool fullscreen = true; for (int j = 3; j < argc; ++j) { if (strcmp(argv[j], \"--debug\") == 0) { debug = true; std::cout << \"Debug mode activated.\" << std::endl; } if (strcmp(argv[j], \"--show\") == 0) { show = true; std::cout << \"Show mode activated.\" << std::endl; } if (strcmp(argv[j], \"--port\") == 0) { listening_port = std::atoi(argv[++j]); std::cout << \"Subscriber will listen to port \" << listening_port << \".\" << std::endl; } if (strcmp(argv[j], \"--no-ncur\") == 0) { ncur_active = false; std::cout << \"Removed ncurses\" << std::endl; } if (strcmp(argv[j], \"--no-fullscreen\") == 0) { fullscreen = false; std::cout << \"Removed fullscreen mode\" << std::endl; } } std::string version = PROJECT_VER; /*************************************** ZMQ ***************************************/ std::string listening_url = \"tcp://127.0.0.1:\" + std::to_string(listening_port); std::string publishing_url = \"tcp://127.0.0.1:9696\"; easy_zmq zmq(27000); void *publisher = zmq.create_publisher(); void *subscriber = zmq.create_subscriber(); zmq.bind_publisher(publisher, publishing_url); zmq.connect_subscriber(subscriber, listening_url, \"\"); zmq.handshake(publisher); /*************************************** 3D part with new library ***************************************/ LibRoyale3D libroyale = LibRoyale3D(); std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl; segmenter *ec = new segmenter(); feature_extractor_3d *fe = new feature_extractor_3d(metrics::L2); for (unsigned int i = 0; i < face_points.size(); ++i) { pcl::PointXYZ point; point.x = face_points.at(i).at(0); point.y = face_points.at(i).at(1); point.z = face_points.at(i).at(2); cloud_person->push_back(point); } /*************************************** 2D part ***************************************/ camera2d camera2d(index); camera2d.open(); bool camera2d_detected = camera2d.detected(); /*************************************** Init ***************************************/ easy_ncurses ncur(version, debug, ncur_active); ncur.write_init(); std::map<std::string, int> bounding_box; cv_show cv_show(\"SmartLock Routine - v\" + version, fullscreen); cv::Mat image; int mb_status; int OK_THRESHOLD = 3; int ok_count = OK_THRESHOLD; std::string previousPerson = \"\"; while (camera2d_detected && libroyale.DETECTED) { try{ if(show && !cv_show.draw_image(image, mb_status)){ break; } ncur.set_face_detection(FACE_DETECTED == 1); ncur.set_face_recognition(FACE_DETECTED == 1, FACE_RECOGNIZED); if(unlocked){ button_pressed = -1; while(button_pressed != 114){ button_pressed = cv::waitKey(1); if(button_pressed == 27){ zmq.close_connection(publisher); zmq.close_connection(subscriber); return 0; } continue; } unlocked = false; } camera2d_detected = camera2d.detected(); ncur.set_cameras_status(camera2d_detected, libroyale.DETECTED); image = camera2d.capture(); cv_show.draw_circles(image, 0); ncur.clear_lines(19, 1); ncur.clear_lines(14, 1); if (!libroyale.LOCKED) { libroyale.lock(); *cloud = libroyale.get_camera_point_cloud(); if (libroyale.has_captured() && cloud->points.size() > 1) { *cloud_clustered = ec->clustering(cloud); std::tuple<float, pcl::PointCloud<pcl::VFHSignature308>> features = fe->get_features(cloud_clustered, cloud_person); float similarity = std::get<0>(features); pcl::PointCloud<pcl::PointXYZ>::iterator begin_pcl = cloud->begin(); pcl::PointCloud<pcl::PointXYZ>::iterator end_pcl = cloud->end(); cloud->erase(begin_pcl, end_pcl); if(debug){ ncur.write_similarity(similarity_, similarity); } if(similarity >= similarity_){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } } else { FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } libroyale.unlock(); } else { FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } FACE_DETECTED = 1; FACE_RECOGNIZED = \"0\"; cv_show.draw_circles(image, 0); // face detection task nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image); mb_status = zmq.send_message(publisher, fd_message.dump()); nlohmann::json fd_response = zmq.receive_message(subscriber); // check if there is only 1 face if (json_key_exists(fd_response, \"data\")){ if(show) { cv_show.draw_bounding_boxes(image, fd_response[\"data\"]); } if(fd_response[\"data\"].size() == 0){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; } else if (fd_response[\"data\"].size() == 1) { ncur.clear_lines(19, 1); int x = fd_response[\"data\"][0][\"boundingBox\"][\"x\"].get<int>(); int y = fd_response[\"data\"][0][\"boundingBox\"][\"y\"].get<int>(); int width = fd_response[\"data\"][0][\"boundingBox\"][\"width\"].get<int>(); int height = fd_response[\"data\"][0][\"boundingBox\"][\"height\"].get<int>(); bounding_box = cv_show.set_bounding_box(x, y, width, height); std::string personFound = \"\"; std::vector<float> landmarks = {0}; cv::Mat cropped_face; if(bounding_box[\"width\"] < cv_show::BB_LIMIT){ FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); } else { // face recognition cropped_face = crop_face(image, bounding_box); if (!cropped_face.empty()) { nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face); mb_status = zmq.send_message(publisher, fr_message.dump()); nlohmann::json fr_response = zmq.receive_message(subscriber); personFound = fr_response[\"data\"][\"prediction\"] == \"unknown\" ? \"\" : fr_response[\"data\"][\"prediction\"]; if(strcmp(personFound.c_str(), \"\") == 0){ // face not recognized // face landmark FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face); mb_status = zmq.send_message(publisher, fl_message.dump()); nlohmann::json fl_response = zmq.receive_message(subscriber); landmarks = fl_response[\"data\"][\"prediction\"].get<std::vector<float>>(); std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug)); int ld_line = 14; ncur.clear_lines(ld_line, 4); ncur.write_line(ld_log, ld_line); } else { // face recognized if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){ if(ok_count-- <= 0){ FACE_RECOGNIZED = personFound; cv_show.draw_thumbnail(image, personFound); cv_show.draw_circles(image, 1); cv_show.draw_name(image, personFound, bounding_box); cv_show.door_unlocked(image); unlocked = true; } } else { ok_count = OK_THRESHOLD; } } } } previousPerson = personFound; } else { // here there are more than 1 face FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; ncur.write_line(\"There must be only 1 face\", 19); } } } catch(std::exception &e){ FACE_DETECTED = 0; FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; libroyale.unlock(); ncur.write_init(); continue; } } // destroy zmq.close_connection(publisher); zmq.close_connection(subscriber); return 0; } Variables Some of the most important variables used into the code are: index : 2D camera index; similarity_ : it sets the 3D threshold to accept something in front of the camera as a face; min_epsilon : it is the similarity threshold for the Face Recognition task; listening_port : it is the port where the software listens to the MuseBox Server responses; ZMQ First of all, we must set the communication between the software and the MuseBox Server using our easy_zmq class. The zmq object receives as input the buffer length, which is set to 27000 by default. std::string listening_url = \"tcp://127.0.0.1:\" + std::to_string(listening_port); std::string publishing_url = \"tcp://127.0.0.1:9696\"; easy_zmq zmq(27000); void *publisher = zmq.create_publisher(); void *subscriber = zmq.create_subscriber(); zmq.bind_publisher(publisher, publishing_url); zmq.connect_subscriber(subscriber, listening_url, \"\"); zmq.handshake(publisher); Publisher The creation of the publisher is entrusted to the appropriate function: void* easy_zmq::create_publisher(){ void* publisher = zmq_socket(context, ZMQ_PUB); assert(publisher); return publisher; } The connection to the listening url is created with the following function: void easy_zmq::bind_publisher(void* publisher, std::string publishing_url){ int bind = zmq_bind(publisher, publishing_url.c_str()); if (bind < 0) { unsigned int error_code = zmq_errno(); std::cout << \"zmq_bind server ctx error: \" << error_code << \", \" << zmq_strerror(error_code) << \"\\n\" << std::endl; assert(bind == 0); } } Subscriber The creation of the subscriber is entrusted to the appropriate function: void* easy_zmq::create_subscriber() { void* subscriber = zmq_socket(context, ZMQ_SUB); assert(subscriber); return subscriber; } The subscriber connection is made by the following function: void easy_zmq::connect_subscriber(void* subscriber, std::string listening_url, std::string topic){ int connect = zmq_connect(subscriber, listening_url.c_str()); if(connect < 0){ std::cout << \"Subscriber not connected\" << std::endl; } int rc = zmq_setsockopt(subscriber, ZMQ_SUBSCRIBE, reinterpret_cast<void*>(&topic), 0); int timeout = 500; zmq_setsockopt(subscriber, ZMQ_RCVTIMEO, &timeout, sizeof(timeout)); } Handshake The handshake phase is necessary to initialize the communication with MuseBox Server. It is made by: void easy_zmq::handshake(void* publisher) { nlohmann::json message = {{\"topic\", \"Handshake\"}}; int out = easy_zmq::send_message(publisher, message.dump()); sleep(1); std::cout << \"Handshake done...\" << std::endl; } that it simply sends a message with the topic set to Handshake . 3D Camera The following code refers to the 3D ToF camera. Please take a look also to 3D APIs section for more information about the 3D APIs used by these pieces of code. We have used a custom library named LibRoyale3D to easily manage the APIs. In our main file the initialization of the 3D camera is made with the code below. In fact, to initialize and make all the needed checks about the 3D camera, we must easily initialize our library with the following call: LibRoyale3D libroyale = LibRoyale3D(); std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl; Once the libroyale variable is defined, we are aware about the 3D camera connection and initialization. Let's deeply analize the content of this library. Most of the functions return a status code which indicates the status of the connection. LibRoyale3D library In this chapter we are going to explain all the functions exposed by our LibRoyale3D library. Constructor First of all, the constructor of our library is able to make some checks and initializations, such as the 3D camera detection, its initialization, the setting of some filters, etc.: LibRoyale3D::LibRoyale3D() { status st = detect(); DETECTED = st == status::STATUS_OK; if(!DETECTED){ return; } st = initialize(); INITD = st == status::STATUS_OK; if(!INITD){ return; } retrieve_streams(); set_filters(); camera->registerDataListener(&depth_data_listener); if(DETECTED && INITD){ start_capture(); } LOCKED = depth_data_listener.guard; INFO = std::string(\"LibRoyale3D - v\") + std::string(LIBROYALE3D_PROJECT_VER) + std::string(\" - MakarenaLabs SRL\"); } 3D camera detection The first action of the construction is the detection of the 3D camera. It is made by the following function: status LibRoyale3D::detect(){ royale::CameraManager manager; royale::Vector<royale::String> camera_list; camera_list = manager.getConnectedCameraList(); if (camera_list.empty()) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"No 3D camera detected\" << std::endl; return status::NO_CAMERA_DETECTED; } camera = manager.createCamera(camera_list[0]); // we have only one camera connected!!! if (camera == nullptr) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Can not create the 3D camera\" << std::endl; return status::NO_CAMERA_CREATED; } std::cout << \"[LIBROYALE3D][INFO] \" << \"3D camera detected\" << std::endl; return status::STATUS_OK; } 3D camera initialization Then, we initialize the camera: status LibRoyale3D::initialize(){ // Before the camera device is ready we have to invoke initialize on it. if (camera->initialize() != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot initialize the 3D camera\" << std::endl; return status::NO_CAMERA_INITIALIZED; } std::cout << \"[LIBROYALE3D][INFO] \" << \"3D camera initialized\" << std::endl; return status::STATUS_OK; } Setting the 3D camera options After the camera initialization, we set the density of the depth information in input: status LibRoyale3D::retrieve_streams(){ if (camera->getStreams(streamIds) != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot retrieve streams from the 3D camera\" << std::endl; return status::NO_CAMERA_STREAMS; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Retrieving streams from the 3D camera\" << std::endl; return status::STATUS_OK; } status LibRoyale3D::set_filters(){ #if LIBROYALE_VERSION == 4 if (camera->setFilterLevel(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS) #else if (camera->setFilterPreset(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS) #endif { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot set filter for stream \" << std::to_string(streamIds[0]) << std::endl; return status::NO_CAMERA_FILTERS; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Set filter for stream \" << std::to_string(streamIds[0]) << std::endl; return status::STATUS_OK; } Starting to capture of the data If all the previous steps end without any problems, we start to obtain the 3D data. To do this, we attach a callback function which will convert the depth information in a Point Cloud and return it for our 3D processing branch: status LibRoyale3D::start_capture(){ // start capture mode if (camera->startCapture() != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Error starting the capturing\" << std::endl; return status::NO_CAMERA_CAPTURE; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Ready to capture data\" << std::endl; return status::STATUS_OK; } Setting the exposure time Finally, we set the exposure time of the camera: status LibRoyale3D::set_exposure_time(int exposure){ if (camera->setExposureTime(exposure, streamIds[0]) != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot set exposure time for stream \" << std::to_string(streamIds[0]) << std::endl; return status::NO_CAMERA_EXPOSURE_TIME; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Changed exposure time for stream \" << std::to_string(streamIds[0]) << \" to \" << exposure << \" microseconds\" << std::endl; exposure_time = exposure; return status::STATUS_OK; } If all the previous tasks run correctly, we have successfully connected the 3D camera: std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl; 2D camera Now, we are ready to initialize the 2D camera using our class, passing as input its index: camera2d camera2d(index); camera2d.open(); bool camera2d_detected = camera2d.detected(); Initialization Previously, we initialize the 2D camera object passing as input its index: camera2d::camera2d(int _index) { index = _index; } Camera aperture Once we have opened the camera, we can start capturing the frames. This piece of code uses OpenCV to init the camera in the cap class global variable and checks whether it is connected or not. Then, it assigns True to the DETECTED class global variable if the camera is ready and opened, False otherwise. void camera2d::open(){ if(index < 0){ index = get_index(15); } cap.open(index); if (!cap.isOpened()) { std::cerr << LOG_2D_CAMERA(\"Cannot open camera\") << std::endl; DETECTED = false; } else { cap.set(cv::CAP_PROP_BUFFERSIZE, 1); try { cv::Mat test_image; cap.read(test_image); DETECTED = true; std::cout << LOG_2D_CAMERA(\"Camera connected\") << std::endl; } catch (...) { std::cerr << LOG_2D_CAMERA(\"Cannot open camera\") << std::endl; DETECTED = false; } } } Processing branches The software processing flow is divided into two parts: 3D processing branch 2D processing and recognition branch Face Detection, Face Recognition and Face Landmark AI tasks are included inside the 2D processing phase. 3D Processing The first procedure which our SmartLock loop executes is the 3D processing branch. This branch receives the Point Cloud from the callback of the 3D camera. The first operation is to segment the face out of the Point Cloud received with the segmenter object: *cloud_clustered = ec->clustering(cloud); The second operation is to extract the features and the similarity of what is in front of the camera to a 3D face std::tuple<float, pcl::PointCloud<pcl::VFHSignature308>> features = fe->get_features(cloud_clustered, cloud_person); float similarity = std::get<0>(features); If the Point Clouds are similar, in front of the camera there is an actual face, and we can proceed, otherwise we restart the loop if(similarity >= similarity_){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } 2D Processing and Recognition The last step, if an actual 3D face is found in front of the camera, is to recognize it. This part of the code does the recognition task, which is composed by: Face Detection; Face Recognition; Face Landmark (to suggest the user to stay straight in front of the camera); Before the 3D processing phase (we did this before to have a continuity in the display of images with OpenCV), we have captured the camera frame using the camera2d class: image = camera2d.capture(); where the function is: cv::Mat camera2d::capture(){ // grab 2D camera frame cv::Mat local_image; cap.read(local_image); cv::resize(local_image, local_image, cv::Size(IMG_WIDTH, IMG_HEIGHT)); cv::putText(local_image, \"Press 'Esc' to close SmartLock Routine\", cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, BLACK, 3); cv::putText(local_image, \"Press 'Esc' to close SmartLock Routine\", cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, WHITE, 1.5); return local_image; } which returns our frame (resized to be compatible with the AI models) as cv::Mat object. With this frame, we can proceed with the entire recognition flow, which is: Fig. 1 - SmartLock Recognition Flow MuseBox messages exchange For all the following task, MuseBox needs the same message's structure. nlohmann::json create_mb_message(std::string topic, std::string publishing_url, std::string listening_url, cv::Mat image, double min_epsilon) { // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".jpg\", image, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); nlohmann::json message = { {\"clientId\", \"1\"}, // client ID {\"topic\", topic}, // Machine Learning task {\"publisherMusebox\", publishing_url}, {\"publisherQueue\", listening_url}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; if (topic != topic_name[topics::FACE_DETECTION]) { message[\"only_face\"] = true; } if (topic == topic_name[topics::FACE_RECOGNITION]) { message[\"min_epsilon\"] = min_epsilon; // face recognition threshold } return message; } In this function, first of all, we need to compress our image before sending it to the server. We do that with: // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".jpg\", image, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); Then, we are able to prepare the JSON request to be sent: nlohmann::json message = { {\"clientId\", \"1\"}, // client ID {\"topic\", topic}, // Machine Learning task {\"publisherMusebox\", publishing_url}, {\"publisherQueue\", listening_url}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; For Face Recognition and Face Landmark tasks, we have to send the only_face flag set to True . Only for Face Recognition task, we have to send the min_epsilon threshold. if (topic != topic_name[topics::FACE_DETECTION]) { message[\"only_face\"] = true; } if (topic == topic_name[topics::FACE_RECOGNITION]) { message[\"min_epsilon\"] = min_epsilon; // face recognition threshold } Now we are ready to send and receive the messages to/from MuseBox Server. To do that, we use two functions: int easy_zmq::send_message(void* publisher, std::string message){ int output = zmq_send(publisher, message.c_str(), message.size(), 0); if (output < 0) { unsigned int error_code = zmq_errno(); std::cout << \"server ctx error: \" << error_code << \", \" << zmq_strerror(error_code) << \"\\n\" << std::endl; } return output; } nlohmann::json easy_zmq::receive_message(void* subscriber){ // blocking receive for ZMQ int nbytes = zmq_recv(subscriber, buffer, buffer_length, 0); std::string str_msg = buffer; if(str_msg == \"\" || str_msg.size() == 0 || nbytes == -1){ str_msg = \"{}\"; } nlohmann::json message = nlohmann::json::parse(str_msg); std::fill_n(buffer, buffer_length, 0); return message; } Face Detection The Face Detection phase starts with the creation of the message to send to the MuseBox Server: // face detection task nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image, 0); mb_status = zmq.send_message(publisher, fd_message.dump()); ncur.clear_lines(19, 1); nlohmann::json fd_response = zmq.receive_message(subscriber); once a face is detected (only 1 face is permitted): if (fd_response[\"data\"].size() == 1) { ncur.clear_lines(19, 1); int x = fd_response[\"data\"][0][\"boundingBox\"][\"x\"].get<int>(); int y = fd_response[\"data\"][0][\"boundingBox\"][\"y\"].get<int>(); int width = fd_response[\"data\"][0][\"boundingBox\"][\"width\"].get<int>(); int height = fd_response[\"data\"][0][\"boundingBox\"][\"height\"].get<int>(); bounding_box = cv_show.set_bounding_box(x, y, width, height); std::string personFound = \"\"; std::vector<float> landmarks = { 0 }; cv::Mat cropped_face; [...] } we save the bounding box's dimensions received from MuseBox Server, and we can go ahead with the next phase. Face Recognition The next step is to send the image of the face detected to the MuseBox Face Recognition task. To do this, we must crop the image following the bounding box's dimensions: cropped_face = crop_face(image, bounding_box); where the crop_face function is: cv::Mat crop_face(cv::Mat image, std::map<std::string, int> bounding_box) { cv::Mat cropped_face; cv::Rect roi; roi.x = bounding_box[\"x\"]; roi.y = bounding_box[\"y\"]; roi.width = bounding_box[\"width\"]; roi.height = bounding_box[\"height\"]; try { cropped_face = image(roi); } catch (cv::Exception &e) { std::cout << \"Couldn't crop image.\" << std::endl; } return cropped_face; } Then, we can set and send the Face Recognition message to the MuseBox Server. If the person is recognized, its name is saved into personFound variable. If the MuseBox Server returns unknown value, personFound variable is set to empty string. nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face, min_epsilon); mb_status = zmq.send_message(publisher, fr_message.dump()); nlohmann::json fr_response = zmq.receive_message(subscriber); personFound = fr_response[\"personFound\"] == \"unknown\" ? \"\" : fr_response[\"personFound\"]; Here two scenarios open up: Face is recognized; Face is not recognized; The detected face is recognized when personFound variable is not empty. In this case, we want to be sure that the recognized person in this turn is the same of the last one. A person is recognized when it is the same for 3 consecutive times ( int OK_THRESHOLD = 3; ). // face recognized if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){ if(ok_count-- <= 0){ FACE_RECOGNIZED = personFound; cv_show.draw_thumbnail(image, personFound); cv_show.draw_circles(image, 1); cv_show.draw_name(image, personFound, bounding_box); } } else { ok_count = OK_THRESHOLD; } Otherwise, if the person is not recognized, it could be they are not facing straight to the camera. To suggest them to do so, we use Face Landmark task. Face Landmark First of all, we must send the message to the MuseBox Server, and we must wait the response: // face not recognized // face landmark FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face, 0); mb_status = zmq.send_message(publisher, fl_message.dump()); nlohmann::json fl_response = zmq.receive_message(subscriber); The MuseBox Server returns the array of landmark points we can use whether to suggest or not to the user to face straight in front of the camera. landmarks = fl_response[\"landmarks\"].get<std::vector<float>>(); std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug)); In the draw_landmarks function, we use three landmark points: Nose Left ear Right ear We check the pan and the roll of the person's face. If the distance between the three points is out of the threshold, we show the suggestion. std::string cv_show::draw_landmarks(cv::Mat image, std::map<std::string, int> bounding_box, std::vector<float> landmarks, bool debug) { std::string to_return = \"\"; std::vector<int> points = {57, 0, 32}; int nose = 57; int ear_sx = 0; int ear_dx = 32; // panning float dx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_dx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_dx*2+1], 2)); float sx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_sx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_sx*2+1], 2)); float pan_threshold = 30; // rolling float ears_distance = std::abs(landmarks[ear_dx*2+1] - landmarks[ear_sx*2+1]); float roll_threshold = 30; if(std::abs(dx_nose_distance - sx_nose_distance) > pan_threshold || ears_distance > roll_threshold){ std::string _msg = \"Please, keep your face straight\"; cv::putText(image, _msg, cv::Point(bounding_box[\"x\"]+10, bounding_box[\"y\"]+bounding_box[\"height\"]-10), cv::FONT_HERSHEY_PLAIN, 1, BLACK, 3); cv::putText(image, _msg, cv::Point(bounding_box[\"x\"]+10, bounding_box[\"y\"]+bounding_box[\"height\"]-10), cv::FONT_HERSHEY_PLAIN, 1, WHITE, 1.5); to_return = \"[WARN] - \" + _msg; } [...] }","title":"Kria KV260"},{"location":"code-analysis/kria-kv260/#kria-kv260-example","text":"","title":"Kria KV260 Example"},{"location":"code-analysis/kria-kv260/#introduction","text":"We will now paste the code of the example application of the SmartLock running on a Kria SOM (in our case a KV260 board) and explain how it works piece by piece. The project is composed of 4 main classes: easy_zmq , which implements the communication between the software and MuseBox Server using ZMQ library; easy_ncurses , which is used to manage the information showed in the console window; cv_show , which exposes some functions to draw a graphic interface; camera2d , which handles 2D camera functions; We use ncurses library to have a better output displayed in the terminal window. Then, we now dig step by step and explain all the software functionalities. Click to expand the entire main.cpp content // #include <royale.hpp> #include \"../include/utils.hpp\" #include \"../include/camera2d.hpp\" #include \"../include/cv_show.hpp\" #include \"../include/3d_face_points.hpp\" #include \"../include/easy_zmq.hpp\" #include \"../include/easy_ncurses.hpp\" #include \"../include/point_cloud/feature_extractor_3d.hpp\" #include \"../include/point_cloud/downsampler.hpp\" #include \"../include/point_cloud/segmenter.hpp\" #include \"../include/libroyale3d/LibRoyale3D.hpp\" pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_person(new pcl::PointCloud<pcl::PointXYZ>()); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_clustered(new pcl::PointCloud<pcl::PointXYZ>()); int main(int argc, char *argv[]) { if (argc < 3) { std::cerr << \" need as input the number of the camera and similarity\" << std::endl; } unsigned int index = std::atoi(argv[1]); float similarity_ = std::atof(argv[2]); int listening_port = 5556; bool unlocked = false; int button_pressed = -1; bool debug = false; bool show = false; bool ncur_active = true; bool fullscreen = true; for (int j = 3; j < argc; ++j) { if (strcmp(argv[j], \"--debug\") == 0) { debug = true; std::cout << \"Debug mode activated.\" << std::endl; } if (strcmp(argv[j], \"--show\") == 0) { show = true; std::cout << \"Show mode activated.\" << std::endl; } if (strcmp(argv[j], \"--port\") == 0) { listening_port = std::atoi(argv[++j]); std::cout << \"Subscriber will listen to port \" << listening_port << \".\" << std::endl; } if (strcmp(argv[j], \"--no-ncur\") == 0) { ncur_active = false; std::cout << \"Removed ncurses\" << std::endl; } if (strcmp(argv[j], \"--no-fullscreen\") == 0) { fullscreen = false; std::cout << \"Removed fullscreen mode\" << std::endl; } } std::string version = PROJECT_VER; /*************************************** ZMQ ***************************************/ std::string listening_url = \"tcp://127.0.0.1:\" + std::to_string(listening_port); std::string publishing_url = \"tcp://127.0.0.1:9696\"; easy_zmq zmq(27000); void *publisher = zmq.create_publisher(); void *subscriber = zmq.create_subscriber(); zmq.bind_publisher(publisher, publishing_url); zmq.connect_subscriber(subscriber, listening_url, \"\"); zmq.handshake(publisher); /*************************************** 3D part with new library ***************************************/ LibRoyale3D libroyale = LibRoyale3D(); std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl; segmenter *ec = new segmenter(); feature_extractor_3d *fe = new feature_extractor_3d(metrics::L2); for (unsigned int i = 0; i < face_points.size(); ++i) { pcl::PointXYZ point; point.x = face_points.at(i).at(0); point.y = face_points.at(i).at(1); point.z = face_points.at(i).at(2); cloud_person->push_back(point); } /*************************************** 2D part ***************************************/ camera2d camera2d(index); camera2d.open(); bool camera2d_detected = camera2d.detected(); /*************************************** Init ***************************************/ easy_ncurses ncur(version, debug, ncur_active); ncur.write_init(); std::map<std::string, int> bounding_box; cv_show cv_show(\"SmartLock Routine - v\" + version, fullscreen); cv::Mat image; int mb_status; int OK_THRESHOLD = 3; int ok_count = OK_THRESHOLD; std::string previousPerson = \"\"; while (camera2d_detected && libroyale.DETECTED) { try{ if(show && !cv_show.draw_image(image, mb_status)){ break; } ncur.set_face_detection(FACE_DETECTED == 1); ncur.set_face_recognition(FACE_DETECTED == 1, FACE_RECOGNIZED); if(unlocked){ button_pressed = -1; while(button_pressed != 114){ button_pressed = cv::waitKey(1); if(button_pressed == 27){ zmq.close_connection(publisher); zmq.close_connection(subscriber); return 0; } continue; } unlocked = false; } camera2d_detected = camera2d.detected(); ncur.set_cameras_status(camera2d_detected, libroyale.DETECTED); image = camera2d.capture(); cv_show.draw_circles(image, 0); ncur.clear_lines(19, 1); ncur.clear_lines(14, 1); if (!libroyale.LOCKED) { libroyale.lock(); *cloud = libroyale.get_camera_point_cloud(); if (libroyale.has_captured() && cloud->points.size() > 1) { *cloud_clustered = ec->clustering(cloud); std::tuple<float, pcl::PointCloud<pcl::VFHSignature308>> features = fe->get_features(cloud_clustered, cloud_person); float similarity = std::get<0>(features); pcl::PointCloud<pcl::PointXYZ>::iterator begin_pcl = cloud->begin(); pcl::PointCloud<pcl::PointXYZ>::iterator end_pcl = cloud->end(); cloud->erase(begin_pcl, end_pcl); if(debug){ ncur.write_similarity(similarity_, similarity); } if(similarity >= similarity_){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } } else { FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } libroyale.unlock(); } else { FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; } FACE_DETECTED = 1; FACE_RECOGNIZED = \"0\"; cv_show.draw_circles(image, 0); // face detection task nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image); mb_status = zmq.send_message(publisher, fd_message.dump()); nlohmann::json fd_response = zmq.receive_message(subscriber); // check if there is only 1 face if (json_key_exists(fd_response, \"data\")){ if(show) { cv_show.draw_bounding_boxes(image, fd_response[\"data\"]); } if(fd_response[\"data\"].size() == 0){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; } else if (fd_response[\"data\"].size() == 1) { ncur.clear_lines(19, 1); int x = fd_response[\"data\"][0][\"boundingBox\"][\"x\"].get<int>(); int y = fd_response[\"data\"][0][\"boundingBox\"][\"y\"].get<int>(); int width = fd_response[\"data\"][0][\"boundingBox\"][\"width\"].get<int>(); int height = fd_response[\"data\"][0][\"boundingBox\"][\"height\"].get<int>(); bounding_box = cv_show.set_bounding_box(x, y, width, height); std::string personFound = \"\"; std::vector<float> landmarks = {0}; cv::Mat cropped_face; if(bounding_box[\"width\"] < cv_show::BB_LIMIT){ FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); } else { // face recognition cropped_face = crop_face(image, bounding_box); if (!cropped_face.empty()) { nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face); mb_status = zmq.send_message(publisher, fr_message.dump()); nlohmann::json fr_response = zmq.receive_message(subscriber); personFound = fr_response[\"data\"][\"prediction\"] == \"unknown\" ? \"\" : fr_response[\"data\"][\"prediction\"]; if(strcmp(personFound.c_str(), \"\") == 0){ // face not recognized // face landmark FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face); mb_status = zmq.send_message(publisher, fl_message.dump()); nlohmann::json fl_response = zmq.receive_message(subscriber); landmarks = fl_response[\"data\"][\"prediction\"].get<std::vector<float>>(); std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug)); int ld_line = 14; ncur.clear_lines(ld_line, 4); ncur.write_line(ld_log, ld_line); } else { // face recognized if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){ if(ok_count-- <= 0){ FACE_RECOGNIZED = personFound; cv_show.draw_thumbnail(image, personFound); cv_show.draw_circles(image, 1); cv_show.draw_name(image, personFound, bounding_box); cv_show.door_unlocked(image); unlocked = true; } } else { ok_count = OK_THRESHOLD; } } } } previousPerson = personFound; } else { // here there are more than 1 face FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; ncur.write_line(\"There must be only 1 face\", 19); } } } catch(std::exception &e){ FACE_DETECTED = 0; FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; libroyale.unlock(); ncur.write_init(); continue; } } // destroy zmq.close_connection(publisher); zmq.close_connection(subscriber); return 0; }","title":"Introduction"},{"location":"code-analysis/kria-kv260/#variables","text":"Some of the most important variables used into the code are: index : 2D camera index; similarity_ : it sets the 3D threshold to accept something in front of the camera as a face; min_epsilon : it is the similarity threshold for the Face Recognition task; listening_port : it is the port where the software listens to the MuseBox Server responses;","title":"Variables"},{"location":"code-analysis/kria-kv260/#zmq","text":"First of all, we must set the communication between the software and the MuseBox Server using our easy_zmq class. The zmq object receives as input the buffer length, which is set to 27000 by default. std::string listening_url = \"tcp://127.0.0.1:\" + std::to_string(listening_port); std::string publishing_url = \"tcp://127.0.0.1:9696\"; easy_zmq zmq(27000); void *publisher = zmq.create_publisher(); void *subscriber = zmq.create_subscriber(); zmq.bind_publisher(publisher, publishing_url); zmq.connect_subscriber(subscriber, listening_url, \"\"); zmq.handshake(publisher);","title":"ZMQ"},{"location":"code-analysis/kria-kv260/#publisher","text":"The creation of the publisher is entrusted to the appropriate function: void* easy_zmq::create_publisher(){ void* publisher = zmq_socket(context, ZMQ_PUB); assert(publisher); return publisher; } The connection to the listening url is created with the following function: void easy_zmq::bind_publisher(void* publisher, std::string publishing_url){ int bind = zmq_bind(publisher, publishing_url.c_str()); if (bind < 0) { unsigned int error_code = zmq_errno(); std::cout << \"zmq_bind server ctx error: \" << error_code << \", \" << zmq_strerror(error_code) << \"\\n\" << std::endl; assert(bind == 0); } }","title":"Publisher"},{"location":"code-analysis/kria-kv260/#subscriber","text":"The creation of the subscriber is entrusted to the appropriate function: void* easy_zmq::create_subscriber() { void* subscriber = zmq_socket(context, ZMQ_SUB); assert(subscriber); return subscriber; } The subscriber connection is made by the following function: void easy_zmq::connect_subscriber(void* subscriber, std::string listening_url, std::string topic){ int connect = zmq_connect(subscriber, listening_url.c_str()); if(connect < 0){ std::cout << \"Subscriber not connected\" << std::endl; } int rc = zmq_setsockopt(subscriber, ZMQ_SUBSCRIBE, reinterpret_cast<void*>(&topic), 0); int timeout = 500; zmq_setsockopt(subscriber, ZMQ_RCVTIMEO, &timeout, sizeof(timeout)); }","title":"Subscriber"},{"location":"code-analysis/kria-kv260/#handshake","text":"The handshake phase is necessary to initialize the communication with MuseBox Server. It is made by: void easy_zmq::handshake(void* publisher) { nlohmann::json message = {{\"topic\", \"Handshake\"}}; int out = easy_zmq::send_message(publisher, message.dump()); sleep(1); std::cout << \"Handshake done...\" << std::endl; } that it simply sends a message with the topic set to Handshake .","title":"Handshake"},{"location":"code-analysis/kria-kv260/#3d-camera","text":"The following code refers to the 3D ToF camera. Please take a look also to 3D APIs section for more information about the 3D APIs used by these pieces of code. We have used a custom library named LibRoyale3D to easily manage the APIs. In our main file the initialization of the 3D camera is made with the code below. In fact, to initialize and make all the needed checks about the 3D camera, we must easily initialize our library with the following call: LibRoyale3D libroyale = LibRoyale3D(); std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl; Once the libroyale variable is defined, we are aware about the 3D camera connection and initialization. Let's deeply analize the content of this library. Most of the functions return a status code which indicates the status of the connection.","title":"3D Camera"},{"location":"code-analysis/kria-kv260/#libroyale3d-library","text":"In this chapter we are going to explain all the functions exposed by our LibRoyale3D library.","title":"LibRoyale3D library"},{"location":"code-analysis/kria-kv260/#constructor","text":"First of all, the constructor of our library is able to make some checks and initializations, such as the 3D camera detection, its initialization, the setting of some filters, etc.: LibRoyale3D::LibRoyale3D() { status st = detect(); DETECTED = st == status::STATUS_OK; if(!DETECTED){ return; } st = initialize(); INITD = st == status::STATUS_OK; if(!INITD){ return; } retrieve_streams(); set_filters(); camera->registerDataListener(&depth_data_listener); if(DETECTED && INITD){ start_capture(); } LOCKED = depth_data_listener.guard; INFO = std::string(\"LibRoyale3D - v\") + std::string(LIBROYALE3D_PROJECT_VER) + std::string(\" - MakarenaLabs SRL\"); }","title":"Constructor"},{"location":"code-analysis/kria-kv260/#3d-camera-detection","text":"The first action of the construction is the detection of the 3D camera. It is made by the following function: status LibRoyale3D::detect(){ royale::CameraManager manager; royale::Vector<royale::String> camera_list; camera_list = manager.getConnectedCameraList(); if (camera_list.empty()) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"No 3D camera detected\" << std::endl; return status::NO_CAMERA_DETECTED; } camera = manager.createCamera(camera_list[0]); // we have only one camera connected!!! if (camera == nullptr) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Can not create the 3D camera\" << std::endl; return status::NO_CAMERA_CREATED; } std::cout << \"[LIBROYALE3D][INFO] \" << \"3D camera detected\" << std::endl; return status::STATUS_OK; }","title":"3D camera detection"},{"location":"code-analysis/kria-kv260/#3d-camera-initialization","text":"Then, we initialize the camera: status LibRoyale3D::initialize(){ // Before the camera device is ready we have to invoke initialize on it. if (camera->initialize() != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot initialize the 3D camera\" << std::endl; return status::NO_CAMERA_INITIALIZED; } std::cout << \"[LIBROYALE3D][INFO] \" << \"3D camera initialized\" << std::endl; return status::STATUS_OK; }","title":"3D camera initialization"},{"location":"code-analysis/kria-kv260/#setting-the-3d-camera-options","text":"After the camera initialization, we set the density of the depth information in input: status LibRoyale3D::retrieve_streams(){ if (camera->getStreams(streamIds) != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot retrieve streams from the 3D camera\" << std::endl; return status::NO_CAMERA_STREAMS; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Retrieving streams from the 3D camera\" << std::endl; return status::STATUS_OK; } status LibRoyale3D::set_filters(){ #if LIBROYALE_VERSION == 4 if (camera->setFilterLevel(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS) #else if (camera->setFilterPreset(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS) #endif { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot set filter for stream \" << std::to_string(streamIds[0]) << std::endl; return status::NO_CAMERA_FILTERS; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Set filter for stream \" << std::to_string(streamIds[0]) << std::endl; return status::STATUS_OK; }","title":"Setting the 3D camera options"},{"location":"code-analysis/kria-kv260/#starting-to-capture-of-the-data","text":"If all the previous steps end without any problems, we start to obtain the 3D data. To do this, we attach a callback function which will convert the depth information in a Point Cloud and return it for our 3D processing branch: status LibRoyale3D::start_capture(){ // start capture mode if (camera->startCapture() != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Error starting the capturing\" << std::endl; return status::NO_CAMERA_CAPTURE; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Ready to capture data\" << std::endl; return status::STATUS_OK; }","title":"Starting to capture of the data"},{"location":"code-analysis/kria-kv260/#setting-the-exposure-time","text":"Finally, we set the exposure time of the camera: status LibRoyale3D::set_exposure_time(int exposure){ if (camera->setExposureTime(exposure, streamIds[0]) != royale::CameraStatus::SUCCESS) { std::cout << \"[LIBROYALE3D][ERROR] \" << \"Cannot set exposure time for stream \" << std::to_string(streamIds[0]) << std::endl; return status::NO_CAMERA_EXPOSURE_TIME; } std::cout << \"[LIBROYALE3D][INFO] \" << \"Changed exposure time for stream \" << std::to_string(streamIds[0]) << \" to \" << exposure << \" microseconds\" << std::endl; exposure_time = exposure; return status::STATUS_OK; } If all the previous tasks run correctly, we have successfully connected the 3D camera: std::cout << LOG_3D_CAMERA(\"Camera connected\") << std::endl;","title":"Setting the exposure time"},{"location":"code-analysis/kria-kv260/#2d-camera","text":"Now, we are ready to initialize the 2D camera using our class, passing as input its index: camera2d camera2d(index); camera2d.open(); bool camera2d_detected = camera2d.detected();","title":"2D camera"},{"location":"code-analysis/kria-kv260/#initialization","text":"Previously, we initialize the 2D camera object passing as input its index: camera2d::camera2d(int _index) { index = _index; }","title":"Initialization"},{"location":"code-analysis/kria-kv260/#camera-aperture","text":"Once we have opened the camera, we can start capturing the frames. This piece of code uses OpenCV to init the camera in the cap class global variable and checks whether it is connected or not. Then, it assigns True to the DETECTED class global variable if the camera is ready and opened, False otherwise. void camera2d::open(){ if(index < 0){ index = get_index(15); } cap.open(index); if (!cap.isOpened()) { std::cerr << LOG_2D_CAMERA(\"Cannot open camera\") << std::endl; DETECTED = false; } else { cap.set(cv::CAP_PROP_BUFFERSIZE, 1); try { cv::Mat test_image; cap.read(test_image); DETECTED = true; std::cout << LOG_2D_CAMERA(\"Camera connected\") << std::endl; } catch (...) { std::cerr << LOG_2D_CAMERA(\"Cannot open camera\") << std::endl; DETECTED = false; } } }","title":"Camera aperture"},{"location":"code-analysis/kria-kv260/#processing-branches","text":"The software processing flow is divided into two parts: 3D processing branch 2D processing and recognition branch Face Detection, Face Recognition and Face Landmark AI tasks are included inside the 2D processing phase.","title":"Processing branches"},{"location":"code-analysis/kria-kv260/#3d-processing","text":"The first procedure which our SmartLock loop executes is the 3D processing branch. This branch receives the Point Cloud from the callback of the 3D camera. The first operation is to segment the face out of the Point Cloud received with the segmenter object: *cloud_clustered = ec->clustering(cloud); The second operation is to extract the features and the similarity of what is in front of the camera to a 3D face std::tuple<float, pcl::PointCloud<pcl::VFHSignature308>> features = fe->get_features(cloud_clustered, cloud_person); float similarity = std::get<0>(features); If the Point Clouds are similar, in front of the camera there is an actual face, and we can proceed, otherwise we restart the loop if(similarity >= similarity_){ FACE_DETECTED = 0; ok_count = OK_THRESHOLD; libroyale.unlock(); continue; }","title":"3D Processing"},{"location":"code-analysis/kria-kv260/#2d-processing-and-recognition","text":"The last step, if an actual 3D face is found in front of the camera, is to recognize it. This part of the code does the recognition task, which is composed by: Face Detection; Face Recognition; Face Landmark (to suggest the user to stay straight in front of the camera); Before the 3D processing phase (we did this before to have a continuity in the display of images with OpenCV), we have captured the camera frame using the camera2d class: image = camera2d.capture(); where the function is: cv::Mat camera2d::capture(){ // grab 2D camera frame cv::Mat local_image; cap.read(local_image); cv::resize(local_image, local_image, cv::Size(IMG_WIDTH, IMG_HEIGHT)); cv::putText(local_image, \"Press 'Esc' to close SmartLock Routine\", cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, BLACK, 3); cv::putText(local_image, \"Press 'Esc' to close SmartLock Routine\", cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, WHITE, 1.5); return local_image; } which returns our frame (resized to be compatible with the AI models) as cv::Mat object. With this frame, we can proceed with the entire recognition flow, which is: Fig. 1 - SmartLock Recognition Flow","title":"2D Processing and Recognition"},{"location":"code-analysis/kria-kv260/#musebox-messages-exchange","text":"For all the following task, MuseBox needs the same message's structure. nlohmann::json create_mb_message(std::string topic, std::string publishing_url, std::string listening_url, cv::Mat image, double min_epsilon) { // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".jpg\", image, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); nlohmann::json message = { {\"clientId\", \"1\"}, // client ID {\"topic\", topic}, // Machine Learning task {\"publisherMusebox\", publishing_url}, {\"publisherQueue\", listening_url}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; if (topic != topic_name[topics::FACE_DETECTION]) { message[\"only_face\"] = true; } if (topic == topic_name[topics::FACE_RECOGNITION]) { message[\"min_epsilon\"] = min_epsilon; // face recognition threshold } return message; } In this function, first of all, we need to compress our image before sending it to the server. We do that with: // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".jpg\", image, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); Then, we are able to prepare the JSON request to be sent: nlohmann::json message = { {\"clientId\", \"1\"}, // client ID {\"topic\", topic}, // Machine Learning task {\"publisherMusebox\", publishing_url}, {\"publisherQueue\", listening_url}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; For Face Recognition and Face Landmark tasks, we have to send the only_face flag set to True . Only for Face Recognition task, we have to send the min_epsilon threshold. if (topic != topic_name[topics::FACE_DETECTION]) { message[\"only_face\"] = true; } if (topic == topic_name[topics::FACE_RECOGNITION]) { message[\"min_epsilon\"] = min_epsilon; // face recognition threshold } Now we are ready to send and receive the messages to/from MuseBox Server. To do that, we use two functions: int easy_zmq::send_message(void* publisher, std::string message){ int output = zmq_send(publisher, message.c_str(), message.size(), 0); if (output < 0) { unsigned int error_code = zmq_errno(); std::cout << \"server ctx error: \" << error_code << \", \" << zmq_strerror(error_code) << \"\\n\" << std::endl; } return output; } nlohmann::json easy_zmq::receive_message(void* subscriber){ // blocking receive for ZMQ int nbytes = zmq_recv(subscriber, buffer, buffer_length, 0); std::string str_msg = buffer; if(str_msg == \"\" || str_msg.size() == 0 || nbytes == -1){ str_msg = \"{}\"; } nlohmann::json message = nlohmann::json::parse(str_msg); std::fill_n(buffer, buffer_length, 0); return message; }","title":"MuseBox messages exchange"},{"location":"code-analysis/kria-kv260/#face-detection","text":"The Face Detection phase starts with the creation of the message to send to the MuseBox Server: // face detection task nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image, 0); mb_status = zmq.send_message(publisher, fd_message.dump()); ncur.clear_lines(19, 1); nlohmann::json fd_response = zmq.receive_message(subscriber); once a face is detected (only 1 face is permitted): if (fd_response[\"data\"].size() == 1) { ncur.clear_lines(19, 1); int x = fd_response[\"data\"][0][\"boundingBox\"][\"x\"].get<int>(); int y = fd_response[\"data\"][0][\"boundingBox\"][\"y\"].get<int>(); int width = fd_response[\"data\"][0][\"boundingBox\"][\"width\"].get<int>(); int height = fd_response[\"data\"][0][\"boundingBox\"][\"height\"].get<int>(); bounding_box = cv_show.set_bounding_box(x, y, width, height); std::string personFound = \"\"; std::vector<float> landmarks = { 0 }; cv::Mat cropped_face; [...] } we save the bounding box's dimensions received from MuseBox Server, and we can go ahead with the next phase.","title":"Face Detection"},{"location":"code-analysis/kria-kv260/#face-recognition","text":"The next step is to send the image of the face detected to the MuseBox Face Recognition task. To do this, we must crop the image following the bounding box's dimensions: cropped_face = crop_face(image, bounding_box); where the crop_face function is: cv::Mat crop_face(cv::Mat image, std::map<std::string, int> bounding_box) { cv::Mat cropped_face; cv::Rect roi; roi.x = bounding_box[\"x\"]; roi.y = bounding_box[\"y\"]; roi.width = bounding_box[\"width\"]; roi.height = bounding_box[\"height\"]; try { cropped_face = image(roi); } catch (cv::Exception &e) { std::cout << \"Couldn't crop image.\" << std::endl; } return cropped_face; } Then, we can set and send the Face Recognition message to the MuseBox Server. If the person is recognized, its name is saved into personFound variable. If the MuseBox Server returns unknown value, personFound variable is set to empty string. nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face, min_epsilon); mb_status = zmq.send_message(publisher, fr_message.dump()); nlohmann::json fr_response = zmq.receive_message(subscriber); personFound = fr_response[\"personFound\"] == \"unknown\" ? \"\" : fr_response[\"personFound\"]; Here two scenarios open up: Face is recognized; Face is not recognized; The detected face is recognized when personFound variable is not empty. In this case, we want to be sure that the recognized person in this turn is the same of the last one. A person is recognized when it is the same for 3 consecutive times ( int OK_THRESHOLD = 3; ). // face recognized if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){ if(ok_count-- <= 0){ FACE_RECOGNIZED = personFound; cv_show.draw_thumbnail(image, personFound); cv_show.draw_circles(image, 1); cv_show.draw_name(image, personFound, bounding_box); } } else { ok_count = OK_THRESHOLD; } Otherwise, if the person is not recognized, it could be they are not facing straight to the camera. To suggest them to do so, we use Face Landmark task.","title":"Face Recognition"},{"location":"code-analysis/kria-kv260/#face-landmark","text":"First of all, we must send the message to the MuseBox Server, and we must wait the response: // face not recognized // face landmark FACE_RECOGNIZED = \"0\"; ok_count = OK_THRESHOLD; cv_show.draw_circles(image, 2); nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face, 0); mb_status = zmq.send_message(publisher, fl_message.dump()); nlohmann::json fl_response = zmq.receive_message(subscriber); The MuseBox Server returns the array of landmark points we can use whether to suggest or not to the user to face straight in front of the camera. landmarks = fl_response[\"landmarks\"].get<std::vector<float>>(); std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug)); In the draw_landmarks function, we use three landmark points: Nose Left ear Right ear We check the pan and the roll of the person's face. If the distance between the three points is out of the threshold, we show the suggestion. std::string cv_show::draw_landmarks(cv::Mat image, std::map<std::string, int> bounding_box, std::vector<float> landmarks, bool debug) { std::string to_return = \"\"; std::vector<int> points = {57, 0, 32}; int nose = 57; int ear_sx = 0; int ear_dx = 32; // panning float dx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_dx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_dx*2+1], 2)); float sx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_sx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_sx*2+1], 2)); float pan_threshold = 30; // rolling float ears_distance = std::abs(landmarks[ear_dx*2+1] - landmarks[ear_sx*2+1]); float roll_threshold = 30; if(std::abs(dx_nose_distance - sx_nose_distance) > pan_threshold || ears_distance > roll_threshold){ std::string _msg = \"Please, keep your face straight\"; cv::putText(image, _msg, cv::Point(bounding_box[\"x\"]+10, bounding_box[\"y\"]+bounding_box[\"height\"]-10), cv::FONT_HERSHEY_PLAIN, 1, BLACK, 3); cv::putText(image, _msg, cv::Point(bounding_box[\"x\"]+10, bounding_box[\"y\"]+bounding_box[\"height\"]-10), cv::FONT_HERSHEY_PLAIN, 1, WHITE, 1.5); to_return = \"[WARN] - \" + _msg; } [...] }","title":"Face Landmark"},{"location":"examples/how-to-run/","text":"How to run demos on the boards This is a video of the setup on the Kria KV260 board. For the Arty Z7-20 the plug-in camera flow is the same. Miscellaneous requirements Before running the solution on the board of your choice, firstly you have to prepare: 2D USB Camera ( Logitech C310HD or Logitech C505 camera recommended); 3D Camera PMD Flexx2_VGA ; USB 3.2 capable USB Type-C Male to USB Type-A Male Data cable: this cable must be a USB 3.2 Data cable, not simply a charging cable. The correct cable is noticeably thick (an example of one cable model which works). Please note that if your workstation\u2019s USB 3.2 connectors use USB Type-C instead of Type-A, then you will need a USB 3.2 capable USB Type-C to USB Type-C Data cable; MicroSD to SD adapter; Small tripod with clamp to hold cameras; Ethernet cable; Network router; How to setup the cameras The following photo shows the camera setup and how the face should be placed. The important factor is that the two cameras are \"parallel\" and looking in the same direction (it works better if the two lens are placed one on top of each other). The face should be placed in the right distance (about 20 to 30 centimeters) away from the cameras. The face (and possibly the eyes) must look in front of the cameras. Make sure you place the 3D camera with the cable on its left , as you can see in the photo at the end of this paragraph. To recap, the key points to run correctly the demo are: The two camera sensors must be aligned; The face should be placed 20 to 30 centimeters away from the cameras; The face must be straight to both cameras; Fig. 2 - Cameras Setup Running examples Moving forward, we provide some instructions about running the examples on these two boards: Kria KV260 Arty Z7-20 Running on Windows If you are planning to run the demo in a Windows environment, please be sure to have all requirements needed and described in the Hardware & Software requirements chapter.","title":"How to run the demo"},{"location":"examples/how-to-run/#how-to-run-demos-on-the-boards","text":"This is a video of the setup on the Kria KV260 board. For the Arty Z7-20 the plug-in camera flow is the same.","title":"How to run demos on the boards"},{"location":"examples/how-to-run/#miscellaneous-requirements","text":"Before running the solution on the board of your choice, firstly you have to prepare: 2D USB Camera ( Logitech C310HD or Logitech C505 camera recommended); 3D Camera PMD Flexx2_VGA ; USB 3.2 capable USB Type-C Male to USB Type-A Male Data cable: this cable must be a USB 3.2 Data cable, not simply a charging cable. The correct cable is noticeably thick (an example of one cable model which works). Please note that if your workstation\u2019s USB 3.2 connectors use USB Type-C instead of Type-A, then you will need a USB 3.2 capable USB Type-C to USB Type-C Data cable; MicroSD to SD adapter; Small tripod with clamp to hold cameras; Ethernet cable; Network router;","title":"Miscellaneous requirements"},{"location":"examples/how-to-run/#how-to-setup-the-cameras","text":"The following photo shows the camera setup and how the face should be placed. The important factor is that the two cameras are \"parallel\" and looking in the same direction (it works better if the two lens are placed one on top of each other). The face should be placed in the right distance (about 20 to 30 centimeters) away from the cameras. The face (and possibly the eyes) must look in front of the cameras. Make sure you place the 3D camera with the cable on its left , as you can see in the photo at the end of this paragraph. To recap, the key points to run correctly the demo are: The two camera sensors must be aligned; The face should be placed 20 to 30 centimeters away from the cameras; The face must be straight to both cameras; Fig. 2 - Cameras Setup","title":"How to setup the cameras"},{"location":"examples/how-to-run/#running-examples","text":"Moving forward, we provide some instructions about running the examples on these two boards: Kria KV260 Arty Z7-20","title":"Running examples"},{"location":"examples/how-to-run/#running-on-windows","text":"If you are planning to run the demo in a Windows environment, please be sure to have all requirements needed and described in the Hardware & Software requirements chapter.","title":"Running on Windows"},{"location":"examples/running-on-arty-z7/","text":"Running the example on the Arty Z7-20 board Fig. 1 - Arty Z7-20 Setting up the board The demo, in addition to the miscellaneous listed in the How to run the demo page, is composed by: Zynq7000 board ( Arty Z7-20 Board , using the Zynq7020); Linux Workstation; MakarenaLabs evaluation image file for Arty (please write to staff@makarenalabs.com to get the access); A MicroSD card greater than or equal to 16GB; Image File Installation Download the MakarenaLabs image and, if it is necessary, merge the downloaded image files into a single image file using 7Zip. Verify the SHA-256 hash of the image file. This will ensure that no errors occurred in the transmission and merge of the files. If your system is Windows 11: Click Search and run powershell . (Note: cmd will not work). Navigate to the directory containing the image file. Enter Get-FileHash <filename> . Wait 15 to 30 seconds. The hash value will appear. Flash the MakarenaLabs image file to the microSD card with Balena Etcher (or another similar one). Please note that the image must be written to the microSD card as a boot.image. Next, connect the Arty to your router with an Ethernet cable. The board is configured to boot with DHCP, thus it should be dynamically assigned an IP by your router. Determine the IP addresses of your Arty board and also your workstation, using your router's management software. Some routers make the software available by typing 192.168.1.1 in your web browser. If you the router's management software is not accessible or available, it has also a static IP address ( 192.168.2.99 ) you can use to connect to. In this case, you have to be in the same subnetwork of the Arty Z7, so set your network adapter properly. The addresess are needed to run the demo. To power the Arty simply connect a micro-USB cable from the board to your PC or to a USB power adapter. Populate Faces Database Connect the 2D USB webcam to the Arty z7, and the 3D camera to a USB 3.0 port of your PC. The first step to run the demo (optional, but strongly suggested for testing) is to add your face to the database. Check the Arty z7 IP address, then open your browser and tap [ip address of board]:9090 (i.e. 192.168.2.99:9090 ). This will open a Jupyter Notebooks environment. Navigate to the folder smart-lock/driver . Here you will find a notebook called takeAPic , open it. Place in front of the USB camera of the setup, and then tap three times Shift+Enter to executes the three cells of the notebook. Stay in front of the camera for 10-20s, as the startup of the Notebook requires some time. After taking the pic the Notebook will ask for the filename to save in the database, then press enter to save it. Write the name but do not insert any extension in the name (i.e. guglielmo and not guglielmo.jpg ). At this point check inside the folder named database to verify that the photo of your face exists with the name you assigned to it. At this point you can close the Web Tab. Start recognition process Open a terminal and connect to the board via SSH. To do so you need to tap into the terminal: ssh xilinx@[ip address of Arty] It will ask you the password. The password is xilinx . To navigate to the folder where the script is placed, execute: cd jupyter_notebooks/smart-lock/driver To start the routine, you must run the next step with sudo privileges. So: sudo /usr/local/share/pynq-venv/bin/python smart_lock_arty.py --ip [your_workstation's_IP] --camera_index -1 You must set the IP address of your workstation and the camera index value, which is -1 so the OS is able to choose the camera index independently. This will start the routine of smart lock which is now waiting to have input fed from your PC. The python script is ready when the log Start Smart Lock Routine appears. Start 3D software on your Workstation First of all, you must configure the tool inside your workstation. Only the steps for a Linux environment are provided, because the Windows version is not currently available. Linux Version Go to the path where you have downloaded the libroyal SDK provided by MakarenaLabs (please contact us at staff@makarenalabs.com if you do not have the SDK) and go to the build folder: cd [/path/to/your/libroyale]/samples/to_arty_7z/build Please be sure to have these prerequisites to continue (we are currently refer to an Ubuntu distro, please check the right commands for your platform): OpenCV Point Cloud Libraries ZeroMQ You can install them running: sudo apt update sudo apt install -y libopencv-dev libpcl-dev libzmq3-dev If all the requirements are met, then you can proceed to configure the tool with: cmake .. make -j8 At this time, you are able to start the 3D software in your workstation. So, you can start the 3D software running ./smart_lock_sender 192.168.2.99 Now the routine of SmartLock should start and print out status messages to Arty console which describes what is happening and whether the Arty and Workstation are exchanging messages. Windows Version Currently not supported","title":"Running on Arty Z7-20"},{"location":"examples/running-on-arty-z7/#running-the-example-on-the-arty-z7-20-board","text":"Fig. 1 - Arty Z7-20","title":"Running the example on the Arty Z7-20 board"},{"location":"examples/running-on-arty-z7/#setting-up-the-board","text":"The demo, in addition to the miscellaneous listed in the How to run the demo page, is composed by: Zynq7000 board ( Arty Z7-20 Board , using the Zynq7020); Linux Workstation; MakarenaLabs evaluation image file for Arty (please write to staff@makarenalabs.com to get the access); A MicroSD card greater than or equal to 16GB;","title":"Setting up the board"},{"location":"examples/running-on-arty-z7/#image-file-installation","text":"Download the MakarenaLabs image and, if it is necessary, merge the downloaded image files into a single image file using 7Zip. Verify the SHA-256 hash of the image file. This will ensure that no errors occurred in the transmission and merge of the files. If your system is Windows 11: Click Search and run powershell . (Note: cmd will not work). Navigate to the directory containing the image file. Enter Get-FileHash <filename> . Wait 15 to 30 seconds. The hash value will appear. Flash the MakarenaLabs image file to the microSD card with Balena Etcher (or another similar one). Please note that the image must be written to the microSD card as a boot.image. Next, connect the Arty to your router with an Ethernet cable. The board is configured to boot with DHCP, thus it should be dynamically assigned an IP by your router. Determine the IP addresses of your Arty board and also your workstation, using your router's management software. Some routers make the software available by typing 192.168.1.1 in your web browser. If you the router's management software is not accessible or available, it has also a static IP address ( 192.168.2.99 ) you can use to connect to. In this case, you have to be in the same subnetwork of the Arty Z7, so set your network adapter properly. The addresess are needed to run the demo. To power the Arty simply connect a micro-USB cable from the board to your PC or to a USB power adapter.","title":"Image File Installation"},{"location":"examples/running-on-arty-z7/#populate-faces-database","text":"Connect the 2D USB webcam to the Arty z7, and the 3D camera to a USB 3.0 port of your PC. The first step to run the demo (optional, but strongly suggested for testing) is to add your face to the database. Check the Arty z7 IP address, then open your browser and tap [ip address of board]:9090 (i.e. 192.168.2.99:9090 ). This will open a Jupyter Notebooks environment. Navigate to the folder smart-lock/driver . Here you will find a notebook called takeAPic , open it. Place in front of the USB camera of the setup, and then tap three times Shift+Enter to executes the three cells of the notebook. Stay in front of the camera for 10-20s, as the startup of the Notebook requires some time. After taking the pic the Notebook will ask for the filename to save in the database, then press enter to save it. Write the name but do not insert any extension in the name (i.e. guglielmo and not guglielmo.jpg ). At this point check inside the folder named database to verify that the photo of your face exists with the name you assigned to it. At this point you can close the Web Tab.","title":"Populate Faces Database"},{"location":"examples/running-on-arty-z7/#start-recognition-process","text":"Open a terminal and connect to the board via SSH. To do so you need to tap into the terminal: ssh xilinx@[ip address of Arty] It will ask you the password. The password is xilinx . To navigate to the folder where the script is placed, execute: cd jupyter_notebooks/smart-lock/driver To start the routine, you must run the next step with sudo privileges. So: sudo /usr/local/share/pynq-venv/bin/python smart_lock_arty.py --ip [your_workstation's_IP] --camera_index -1 You must set the IP address of your workstation and the camera index value, which is -1 so the OS is able to choose the camera index independently. This will start the routine of smart lock which is now waiting to have input fed from your PC. The python script is ready when the log Start Smart Lock Routine appears.","title":"Start recognition process"},{"location":"examples/running-on-arty-z7/#start-3d-software-on-your-workstation","text":"First of all, you must configure the tool inside your workstation. Only the steps for a Linux environment are provided, because the Windows version is not currently available.","title":"Start 3D software on your Workstation"},{"location":"examples/running-on-arty-z7/#linux-version","text":"Go to the path where you have downloaded the libroyal SDK provided by MakarenaLabs (please contact us at staff@makarenalabs.com if you do not have the SDK) and go to the build folder: cd [/path/to/your/libroyale]/samples/to_arty_7z/build Please be sure to have these prerequisites to continue (we are currently refer to an Ubuntu distro, please check the right commands for your platform): OpenCV Point Cloud Libraries ZeroMQ You can install them running: sudo apt update sudo apt install -y libopencv-dev libpcl-dev libzmq3-dev If all the requirements are met, then you can proceed to configure the tool with: cmake .. make -j8 At this time, you are able to start the 3D software in your workstation. So, you can start the 3D software running ./smart_lock_sender 192.168.2.99 Now the routine of SmartLock should start and print out status messages to Arty console which describes what is happening and whether the Arty and Workstation are exchanging messages.","title":"Linux Version"},{"location":"examples/running-on-arty-z7/#windows-version","text":"Currently not supported","title":"Windows Version"},{"location":"examples/running-on-kria-kv260/","text":"Running the example on the Kria KV260 board Fig. 1 - Kria KV260 Setting up the board The demo, in addition to the miscellaneous listed in the How to run the demo page, is composed by: Face recognition image file, available from MakarenaLabs (please write to staff@makarenalabs.com to get the access); Kria KV260 Board ; A MicroSD card greater than or equal to 16GB; If you are running on a Windows environment, please check if you have all the software requirements needed and described in the Hardware & Software requirements chapter. Board connection For this demo there are two connection ways: With an ethernet cable using your workstation (all hardware requirements should be satisfied); Directly using an external monitor. In this case, as it is written in the hardware and software requirements chapter ) you need also: Full-HD monitor (1920x1080); HDMI cable; Keyboard and mouse kit; When the Kria boots, please check the three LEDs near the fan. The center light should blink and the other two lights should be solid when the board has successfully booted. Fig. 1 - Kria KV260 booting LEDs Ethernet connection If you have chosen to use your local setup to connect to the board, start connecting via ethernet cable the Kria to your router. The board is set with DHCP, thus it should be dynamically assigned an IP from the router. Check the address of your board and PC on your router manager software. Those addresses are necessary to start the demo. Open a terminal and connect to the board via SSH. To do so, you need to run into the terminal: ssh root@[ip address of KRIA] It will ask you the password (type root ). If you entered it correctly, you are successfully connected to the board. Standalone connection If you have chosen to connect directly to the Kria board using an external Full-HD monitor, you must follow these steps, starting with the board off: Connect the Kria to the Full-HD monitor with the HDMI cable; Connect the keyboard and mouse kit to the Kria; Power on the board and wait until the home page is shown on the external monitor; Open the terminal clicking on the Terminal icon in the home page; Now the board is ready to use. Image File Installation Download the MakarenaLabs image. They are 4 files to be merged together with 7Zip; Merge the downloaded image files into a single image file; Verify the SHA-256 hash of the image file. This will ensure that no errors occurred in the transmission and merge of the files. If your system is Windows 11: Click Search and run powershell . (Note: cmd will not work); Navigate to the directory containing the image file; Enter Get-FileHash <filename> ; Wait 15 to 30 seconds; The hash value will appear; Flash the MakarenaLabs image file to the microSD card with Balena Etcher. It must be flashed as a boot image; Connect the 2D USB webcam and the 3D camera to the USB ports of the Kria and wait about 3 or 4 seconds. If you want to know the IDs of the cameras plugged in, run the following command v4l2-ctl --list-devices you will have an ouput similar to the following web camera: XM Camera (usb-xhci-hcd.1.auto-1.1): /dev/video0 /dev/video1 /dev/media0 pmdmodule287xUVC (usb-xhci-hcd.1.auto-1.3): /dev/video2 /dev/video3 /dev/media1 where the ID of the camera we need is the first one. In this case, our 2D camera is the one named web camera: XM Camera with its ID = 0, while the 3D camera is called pmdmodule287xUVC with its ID = 2 (basically you have to take the number at the end of the path, so the web camera has ID 0 because it has the path /dev/video0 which ends with 0). Start the MuseBox Server process The first step to run the demo is to start the MuseBox Server process. Go to the MuseBox's root directory with cd /home/root/smartlock-core-kria-kv260/musebox-server To run the MuseBox Server, tap ./MuseBox zmq The server is ready when the server's port is shown on terminal, for example MuseBox Server - v3.0.0 Checking license... No license needed License ok remove from slot 0 returns: 0 (Ok) kv260-benchmark-b4096: loaded to slot 0 FaceDetection init success. FaceRecognition init success. Database /home/root/smartlock-core-kria-kv260/musebox-server/database/ feature extraction success. FaceLandmark 196 points init success. ZeroMQ selected **************************************************************************** * MuseBox server is running at port 9696! * **************************************************************************** Populate Faces Database The second step (optional, but strongly suggested for testing) is to add your face to the database. You can do that using the MakarenaLabs take-a-pic tool. For this step, please be sure not to have the 3D camera attached to the board in order to avoid some priority problems in the automatic choosing of the camera by the OS. Configuration To configure take-a-pic tool, first open a new terminal and connect to the board with ssh root@[ip address of KRIA] . The password is root . When you are connected to the board, go to the tool's root directory using: cd smartlock-core-kria-kv260/take-a-pic The tool has several configuration flags and parameters that we are going to explain. To run the help function of the tool, run: venv/bin/python takeAPic.py --help and it will show an output like this one usage: Take a Pic [-h] [-c CAMERA_ID] [-sh SUB_HOST] [-ph PUB_HOST] [-sp SUB_PORT] [-pp PUB_PORT] [-v] Take a Pic optional arguments: -h, --help show this help message and exit -c CAMERA_ID, --camera CAMERA_ID Camera ID -sh SUB_HOST, --sub_host SUB_HOST Subscriber Host -ph PUB_HOST, --pub_host PUB_HOST Publisher Host -sp SUB_PORT, --sub_port SUB_PORT Subscriber Port -pp PUB_PORT, --pub_port PUB_PORT Publisher Port -v, --version show program's version number and exit where: -c : you use this flag to set the id of 2D camera which is used to take the pic of your face. Default value is -1 , so the OS can choose by itself the 2D camera; -sh : the host of the subscriber. Default value is 127.0.0.1 ; -ph : the host of the publisher. Default value is 127.0.0.1 ; -sp : the port of the subscriber. Default value is 5556 ; -pp : the port of the publisher. Default value is 9696 ; -v : it is used to show the current version of the tool; Execution Before starting the execution, verify that the 2D camera is attached to a USB port on the Kria board and do not forget to verify its right ID as explained at the top of this section. In our case the ID of 2D camera is 0, so: Execute the script with the command venv/bin/python takeAPic.py -c 0 and place yourself in front of the cameras. Follow the monitor tool instructions. You will have to press the c key to take a pic, or the q one to quit. You should now see a new line asking to insert the name of the file. Please insert it without the file extension (i.e. guglielmo , not guglielmo.png ). Fig. 1 - TakeAPic with 1 person At this step, you have to restart MuseBox Server to recognize the new face: Go back to the same terminal where the server was started and tap Ctrl + C ; To restart MuseBox Server run ./MuseBox zmq ; Start demo software Verify that the 3D camera is attached to a USB port on the Kria using a USB 3.2 data cable. Please note that a common reason for failure is the use of a USB cable rated for power but not for USB 3.2 data. The smart_lock_bin executable accepts two necessary parameters and three optional flags as input. The first value you must send to it is the ID of the 2D camera which you have obtained in the previous steps. The second one is the 3D confidence which indicates the similarity of the face being recorded with a real face model, represented as a point cloud. Basically, the lower is the value of the 3D similarity, the more confident the system is in validating an actual human face. A real face must have a similarity less than or equal to that value to be considered so. By default this value is set to 1800 . Then, the three flags are: --show : with this flag you can enable the user interface in order to see the camera output; --debug : this flag is used to know the state of some variables within the code; --no-fullscreen : this flag avoids the fullscreen mode (not recommended using the standalone connection ); So, for example, if we run the command ./smart_lock_bin 0 1800 --show , we are setting the 2D camera with ID 0 and the 3D threshold to 1800, so our face has to be a similarity less than or equal to that value to be considered so. Let's start with the next steps. On the same terminal of take-a-pic or on a new terminal, repeat the ssh connection process Go to the SmartLock Core build directory with: cd /home/root/smartlock-core-kria-kv260/build Run the smart_lock_bin executable with two parameters: 2D camera index and the 3D confidence. An example might be: ./smart_lock_bin 0 1800 --show with the --show flag the graphic interface can be activated and the images are shown in full screen (by default). When the face is recognized, the flow stucks and asks you to press the r button on the keyboard to restart the recognition flow. This will start the SmartLock algorithm and the results of the AI models will be displayed on this new terminal.","title":"Running on Kria KV260"},{"location":"examples/running-on-kria-kv260/#running-the-example-on-the-kria-kv260-board","text":"Fig. 1 - Kria KV260","title":"Running the example on the Kria KV260 board"},{"location":"examples/running-on-kria-kv260/#setting-up-the-board","text":"The demo, in addition to the miscellaneous listed in the How to run the demo page, is composed by: Face recognition image file, available from MakarenaLabs (please write to staff@makarenalabs.com to get the access); Kria KV260 Board ; A MicroSD card greater than or equal to 16GB; If you are running on a Windows environment, please check if you have all the software requirements needed and described in the Hardware & Software requirements chapter.","title":"Setting up the board"},{"location":"examples/running-on-kria-kv260/#board-connection","text":"For this demo there are two connection ways: With an ethernet cable using your workstation (all hardware requirements should be satisfied); Directly using an external monitor. In this case, as it is written in the hardware and software requirements chapter ) you need also: Full-HD monitor (1920x1080); HDMI cable; Keyboard and mouse kit; When the Kria boots, please check the three LEDs near the fan. The center light should blink and the other two lights should be solid when the board has successfully booted. Fig. 1 - Kria KV260 booting LEDs","title":"Board connection"},{"location":"examples/running-on-kria-kv260/#ethernet-connection","text":"If you have chosen to use your local setup to connect to the board, start connecting via ethernet cable the Kria to your router. The board is set with DHCP, thus it should be dynamically assigned an IP from the router. Check the address of your board and PC on your router manager software. Those addresses are necessary to start the demo. Open a terminal and connect to the board via SSH. To do so, you need to run into the terminal: ssh root@[ip address of KRIA] It will ask you the password (type root ). If you entered it correctly, you are successfully connected to the board.","title":"Ethernet connection"},{"location":"examples/running-on-kria-kv260/#standalone-connection","text":"If you have chosen to connect directly to the Kria board using an external Full-HD monitor, you must follow these steps, starting with the board off: Connect the Kria to the Full-HD monitor with the HDMI cable; Connect the keyboard and mouse kit to the Kria; Power on the board and wait until the home page is shown on the external monitor; Open the terminal clicking on the Terminal icon in the home page; Now the board is ready to use.","title":"Standalone connection"},{"location":"examples/running-on-kria-kv260/#image-file-installation","text":"Download the MakarenaLabs image. They are 4 files to be merged together with 7Zip; Merge the downloaded image files into a single image file; Verify the SHA-256 hash of the image file. This will ensure that no errors occurred in the transmission and merge of the files. If your system is Windows 11: Click Search and run powershell . (Note: cmd will not work); Navigate to the directory containing the image file; Enter Get-FileHash <filename> ; Wait 15 to 30 seconds; The hash value will appear; Flash the MakarenaLabs image file to the microSD card with Balena Etcher. It must be flashed as a boot image; Connect the 2D USB webcam and the 3D camera to the USB ports of the Kria and wait about 3 or 4 seconds. If you want to know the IDs of the cameras plugged in, run the following command v4l2-ctl --list-devices you will have an ouput similar to the following web camera: XM Camera (usb-xhci-hcd.1.auto-1.1): /dev/video0 /dev/video1 /dev/media0 pmdmodule287xUVC (usb-xhci-hcd.1.auto-1.3): /dev/video2 /dev/video3 /dev/media1 where the ID of the camera we need is the first one. In this case, our 2D camera is the one named web camera: XM Camera with its ID = 0, while the 3D camera is called pmdmodule287xUVC with its ID = 2 (basically you have to take the number at the end of the path, so the web camera has ID 0 because it has the path /dev/video0 which ends with 0).","title":"Image File Installation"},{"location":"examples/running-on-kria-kv260/#start-the-musebox-server-process","text":"The first step to run the demo is to start the MuseBox Server process. Go to the MuseBox's root directory with cd /home/root/smartlock-core-kria-kv260/musebox-server To run the MuseBox Server, tap ./MuseBox zmq The server is ready when the server's port is shown on terminal, for example MuseBox Server - v3.0.0 Checking license... No license needed License ok remove from slot 0 returns: 0 (Ok) kv260-benchmark-b4096: loaded to slot 0 FaceDetection init success. FaceRecognition init success. Database /home/root/smartlock-core-kria-kv260/musebox-server/database/ feature extraction success. FaceLandmark 196 points init success. ZeroMQ selected **************************************************************************** * MuseBox server is running at port 9696! * ****************************************************************************","title":"Start the MuseBox Server process"},{"location":"examples/running-on-kria-kv260/#populate-faces-database","text":"The second step (optional, but strongly suggested for testing) is to add your face to the database. You can do that using the MakarenaLabs take-a-pic tool. For this step, please be sure not to have the 3D camera attached to the board in order to avoid some priority problems in the automatic choosing of the camera by the OS.","title":"Populate Faces Database"},{"location":"examples/running-on-kria-kv260/#configuration","text":"To configure take-a-pic tool, first open a new terminal and connect to the board with ssh root@[ip address of KRIA] . The password is root . When you are connected to the board, go to the tool's root directory using: cd smartlock-core-kria-kv260/take-a-pic The tool has several configuration flags and parameters that we are going to explain. To run the help function of the tool, run: venv/bin/python takeAPic.py --help and it will show an output like this one usage: Take a Pic [-h] [-c CAMERA_ID] [-sh SUB_HOST] [-ph PUB_HOST] [-sp SUB_PORT] [-pp PUB_PORT] [-v] Take a Pic optional arguments: -h, --help show this help message and exit -c CAMERA_ID, --camera CAMERA_ID Camera ID -sh SUB_HOST, --sub_host SUB_HOST Subscriber Host -ph PUB_HOST, --pub_host PUB_HOST Publisher Host -sp SUB_PORT, --sub_port SUB_PORT Subscriber Port -pp PUB_PORT, --pub_port PUB_PORT Publisher Port -v, --version show program's version number and exit where: -c : you use this flag to set the id of 2D camera which is used to take the pic of your face. Default value is -1 , so the OS can choose by itself the 2D camera; -sh : the host of the subscriber. Default value is 127.0.0.1 ; -ph : the host of the publisher. Default value is 127.0.0.1 ; -sp : the port of the subscriber. Default value is 5556 ; -pp : the port of the publisher. Default value is 9696 ; -v : it is used to show the current version of the tool;","title":"Configuration"},{"location":"examples/running-on-kria-kv260/#execution","text":"Before starting the execution, verify that the 2D camera is attached to a USB port on the Kria board and do not forget to verify its right ID as explained at the top of this section. In our case the ID of 2D camera is 0, so: Execute the script with the command venv/bin/python takeAPic.py -c 0 and place yourself in front of the cameras. Follow the monitor tool instructions. You will have to press the c key to take a pic, or the q one to quit. You should now see a new line asking to insert the name of the file. Please insert it without the file extension (i.e. guglielmo , not guglielmo.png ). Fig. 1 - TakeAPic with 1 person At this step, you have to restart MuseBox Server to recognize the new face: Go back to the same terminal where the server was started and tap Ctrl + C ; To restart MuseBox Server run ./MuseBox zmq ;","title":"Execution"},{"location":"examples/running-on-kria-kv260/#start-demo-software","text":"Verify that the 3D camera is attached to a USB port on the Kria using a USB 3.2 data cable. Please note that a common reason for failure is the use of a USB cable rated for power but not for USB 3.2 data. The smart_lock_bin executable accepts two necessary parameters and three optional flags as input. The first value you must send to it is the ID of the 2D camera which you have obtained in the previous steps. The second one is the 3D confidence which indicates the similarity of the face being recorded with a real face model, represented as a point cloud. Basically, the lower is the value of the 3D similarity, the more confident the system is in validating an actual human face. A real face must have a similarity less than or equal to that value to be considered so. By default this value is set to 1800 . Then, the three flags are: --show : with this flag you can enable the user interface in order to see the camera output; --debug : this flag is used to know the state of some variables within the code; --no-fullscreen : this flag avoids the fullscreen mode (not recommended using the standalone connection ); So, for example, if we run the command ./smart_lock_bin 0 1800 --show , we are setting the 2D camera with ID 0 and the 3D threshold to 1800, so our face has to be a similarity less than or equal to that value to be considered so. Let's start with the next steps. On the same terminal of take-a-pic or on a new terminal, repeat the ssh connection process Go to the SmartLock Core build directory with: cd /home/root/smartlock-core-kria-kv260/build Run the smart_lock_bin executable with two parameters: 2D camera index and the 3D confidence. An example might be: ./smart_lock_bin 0 1800 --show with the --show flag the graphic interface can be activated and the images are shown in full screen (by default). When the face is recognized, the flow stucks and asks you to press the r button on the keyboard to restart the recognition flow. This will start the SmartLock algorithm and the results of the AI models will be displayed on this new terminal.","title":"Start demo software"},{"location":"testbench/kv260/","text":"Kria KV260 Testbench This is a guide on how to validate the FaceRecognition AI model on the KV260 board. To run the validation testbench you need to have: Kria KV260 board; Ubuntu 22.04 image (username: ubuntu , password: makarenalabs ); MuseBox License @MakarenaLabs ; The MuseBox License is really important, without it you won't be able to run the testbench. In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, so you cannot use the license on a different node. For the license request, you need to create the license request file. To do that, simply run this command: sudo license_request This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request and with the message body I accept your evaluation license agreement . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin . Once you have your license, you should go to a browser and go to http://<kria_board_ip>:9090 . The password to insert is xilinx . Go inside the MuseBox folder and open validate_face_recognition.ipynb . Now you can run all the cells and run the testbench. When you run the first cell: import cv2 import os import numpy as np from FaceRecognitionSL import FaceRecognition you should see as output: Checking license... License ok On the other hand, if you get any error and the kernel seems to die, it may be an error with the license. You can change the maximum length of the images to run the validation on, by changing the variable: max_len_images = 1000 and you can change the Face Recognition similarity threshold by changing the variable: threshold = 0.12","title":"Kria KV260"},{"location":"testbench/kv260/#kria-kv260-testbench","text":"This is a guide on how to validate the FaceRecognition AI model on the KV260 board. To run the validation testbench you need to have: Kria KV260 board; Ubuntu 22.04 image (username: ubuntu , password: makarenalabs ); MuseBox License @MakarenaLabs ; The MuseBox License is really important, without it you won't be able to run the testbench. In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, so you cannot use the license on a different node. For the license request, you need to create the license request file. To do that, simply run this command: sudo license_request This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request and with the message body I accept your evaluation license agreement . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin . Once you have your license, you should go to a browser and go to http://<kria_board_ip>:9090 . The password to insert is xilinx . Go inside the MuseBox folder and open validate_face_recognition.ipynb . Now you can run all the cells and run the testbench. When you run the first cell: import cv2 import os import numpy as np from FaceRecognitionSL import FaceRecognition you should see as output: Checking license... License ok On the other hand, if you get any error and the kernel seems to die, it may be an error with the license. You can change the maximum length of the images to run the validation on, by changing the variable: max_len_images = 1000 and you can change the Face Recognition similarity threshold by changing the variable: threshold = 0.12","title":"Kria KV260 Testbench"}]}