<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://smartlock.musebox.it/code-analysis/arty-z7/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Arty Z7-20 - SmartLock Design Reference</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Arty Z7-20";
        var mkdocs_page_input_path = "code-analysis/arty-z7.md";
        var mkdocs_page_url = "/code-analysis/arty-z7/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> SmartLock Design Reference
        </a>
        <div class="version">
          v2.0.4
        </div><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Introduction</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../project-description/">Project Description</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../hw-sw-requirements/">Hardware & Software requirements</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Run the examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/how-to-run/">How to run the demo</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/running-on-kria-kv260/">Running on Kria KV260</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/running-on-arty-z7/">Running on Arty Z7-20</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Code analysis</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../kria-kv260/">Kria KV260</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Arty Z7-20</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#run-face-recognition">Run Face Recognition</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#tcp-connection-with-an-external-workstation">TCP connection with an external workstation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#load-the-photos-to-be-recognized">Load the photos to be recognized</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#main-loop">Main Loop</a>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Testbench</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../testbench/kv260/">Kria KV260</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">SmartLock Design Reference</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Code analysis</li>
      <li class="breadcrumb-item active">Arty Z7-20</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="smartlock-example-on-zynq7000">SmartLock example on Zynq7000</h1>
<h2 id="introduction">Introduction</h2>
<p>We will now paste the code of the example application of the SmartLock running on a Zynq7000 (in our case an Arty-7Z020 board) and explain how it works piece by piece.</p>
<pre><code class="language-python">#!/usr/bin/env python
# coding: utf-8

import numpy as np
from driver import io_shape_dict
from driver_base import FINNExampleOverlay
import time
import cv2
import zmq
import json
import base64
import imageio
import io
import os

import musebox_pcl_bindings_feature_extractor as fe3d
import musebox_pcl_bindings_segmenter as ec

import argparse
from yunet import YuNet

mean = [0.6071, 0.4609, 0.3944]
std = [0.2457, 0.2175, 0.2129]

database_dict_list = []

# Load the cascade
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

bsize = 1
bitfile = &quot;/home/xilinx/jupyter_notebooks/smart-lock/bitfile/smartlock.bit&quot;
weights = &quot;/home/xilinx/jupyter_notebooks/smart-lock/driver/runtime_weights/&quot;
platform = &quot;zynq-iodma&quot;


driver = FINNExampleOverlay(
    bitfile_name=bitfile,
    platform=platform,
    io_shape_dict=io_shape_dict,
    batch_size=bsize,
    runtime_weight_dir=weights,
)

def run(image_input):
    global driver, mean, std
    image_input = image_input.astype(np.float32)
    image_input[:,:,0] /= 255
    image_input[:,:,1] /= 255
    image_input[:,:,2] /= 255
    image_input = cv2.resize(image_input, (140, 140))
    image_input[:,:,0] -= mean[0]
    image_input[:,:,1] -= mean[1]
    image_input[:,:,2] -= mean[2]

    image_input[:,:,0] /= std[0]
    image_input[:,:,1] /= std[1]
    image_input[:,:,2] /= std[2]
    image_input *= 255
    ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape)
    driver.copy_input_data_to_device(ibuf_normal)
    driver.execute_on_buffers()
    obuf_normal = np.empty_like(driver.obuf_packed_device[0])
    driver.copy_output_data_from_device(obuf_normal)
    obuf_folded = driver.unpack_output(obuf_normal)
    final_output = driver.unfold_output(obuf_folded)
    return final_output

# ip 192.168.188.107
def init_sender(ip=&quot;tcp://*:5557&quot;):
    global context, socket_pub
    ############## SEND #############
    socket_pub = context.socket(zmq.PUB)
    socket_pub.bind(&quot;tcp://*:5557&quot;)


context = zmq.Context()
socket = None
socket_pub = None

def init_listener(ip=&quot;tcp://192.168.188.60:5556&quot;):
    global context, socket
    # CREATE COMMUNICATION LISTENER
    # context = zmq.Context()
    ############## RECEIVE #############
    socket = context.socket(zmq.SUB)
    socket.connect(ip)
    socket.setsockopt_string(zmq.SUBSCRIBE, str(''))


def get_data():
    global context, socket
    message = None
    i = 0
    while i &lt; 1:
        try:
            zmq_response = socket.recv()
            # print(&quot;has received&quot;)
            message = json.loads(zmq_response)
            i = i + 1
        except Exception as e:
            print(&quot;Exception: &quot;, e)
    return message


def send_trigger(trigger: str):
   global socket_pub

   message = {&quot;trigger&quot;: trigger}
   socket_pub.send_string(json.dumps(message))


def init_database(folder: str):
    global database_dict_list
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder, filename))
        features = run(img)
        if img is not None:
            database_dict_list.append({&quot;features&quot;: features, &quot;label&quot;: filename})

def init(ip_address: str):

    ip_address_ = &quot;tcp://&quot; + ip_address + &quot;:5556&quot;

    init_listener(ip_address_)

    init_sender()

    init_database(&quot;database&quot;)

def main_sl(ip_address: str, camera_index: int):

    init(ip_address)

    model = YuNet(modelPath=&quot;face_detection_yunet_2022mar.onnx&quot;,
                  inputSize=[320, 320],
                  confThreshold=0.9,
                  nmsThreshold=0.3,
                  topK=5000,
                  )
    try:
        cap.release()
    except:
        pass
    cap = cv2.VideoCapture(camera_index)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    pcl_similarity = 2500
    fr_similarity = 700

    aspect_ratio = (320/320, 320/320)

    print(&quot;Start Smart Lock Routine&quot;)

    while True:

        message_recv = get_data()
        _, img = cap.read()

        if img is None:
            print(&quot;I can't read from the camera!&quot;)
            continue

        result_segmenter = ec.euclidean_clustering(message_recv[&quot;point_cloud_camera&quot;])
        res = fe3d.feature_extractor_3d(result_segmenter, message_recv[&quot;point_cloud_database&quot;])

        if res &gt; pcl_similarity:
            print(&quot;NOT A PERSON!&quot;)
            print(res)
            send_trigger(&quot;continue&quot;)
            continue

        sim_ = 99999
        person_ = &quot;&quot;
        try:
            image = img.copy()
            image = cv2.resize(image, (320, 320))
            img = cv2.resize(img, (320, 320))
            model.setInputSize([320, 320])
            result = model.infer(img)
            output = None
            for det in result:
                bbox = det[0:4].astype(np.int32)
                output = bbox
                break

            image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])]

            features = run(image)
            for elem in database_dict_list:
                sim_face = np.linalg.norm(features - elem[&quot;features&quot;])

                if sim_face &lt; fr_similarity:
                    if sim_face &lt; sim_:
                        sim_ = sim_face
                        person_ = elem[&quot;label&quot;]
            if sim_ != 99999:
                sim_ = 99999
                print(&quot;person is: &quot;, person_)
            else:
                print(&quot;UNKOWN&quot;)

            send_trigger(&quot;continue&quot;)
        except:
            send_trigger(&quot;continue&quot;)
            pass


if __name__ == &quot;__main__&quot;:
    # Create the parser
    parser = argparse.ArgumentParser()# Add an argument
    parser.add_argument('--ip', type=str, required=True)# Parse the argument
    parser.add_argument('--camera_index', type=int, required=True)# Parse the argument
    args = parser.parse_args()
    main_sl(args.ip, args.camera_index)
</code></pre>
<p>We now dig function per function and explain its functionality.</p>
<h2 id="run-face-recognition">Run Face Recognition</h2>
<pre><code class="language-python">def run(image_input):
    global driver, mean, std
    image_input = image_input.astype(np.float32)
    image_input[:,:,0] /= 255
    image_input[:,:,1] /= 255
    image_input[:,:,2] /= 255
    image_input = cv2.resize(image_input, (140, 140))
    image_input[:,:,0] -= mean[0]
    image_input[:,:,1] -= mean[1]
    image_input[:,:,2] -= mean[2]

    image_input[:,:,0] /= std[0]
    image_input[:,:,1] /= std[1]
    image_input[:,:,2] /= std[2]
    image_input *= 255
    ibuf_normal = image_input.astype(np.uint8).reshape(driver.ibuf_packed_device[0].shape)
    driver.copy_input_data_to_device(ibuf_normal)
    driver.execute_on_buffers()
    obuf_normal = np.empty_like(driver.obuf_packed_device[0])
    driver.copy_output_data_from_device(obuf_normal)
    obuf_folded = driver.unpack_output(obuf_normal)
    final_output = driver.unfold_output(obuf_folded)
    return final_output
</code></pre>
<p>This function takes in input a bounding box processed using the python bindings of OpenCV and return the features that describe it. The feature extraction is performed using our custom Face Recognition model. The image normalization is mandatory before inferring the model, but is taken care from the function.</p>
<h2 id="tcp-connection-with-an-external-workstation">TCP connection with an external workstation</h2>
<pre><code class="language-python">
def init_sender(ip=&quot;tcp://*:5557&quot;):
    global context, socket_pub
    ############## SEND #############
    socket_pub = context.socket(zmq.PUB)
    socket_pub.bind(&quot;tcp://*:5557&quot;)


context = zmq.Context()
socket = None
socket_pub = None

def init_listener(ip=&quot;tcp://192.168.188.60:5556&quot;):
    global context, socket
    # CREATE COMMUNICATION LISTENER
    # context = zmq.Context()
    ############## RECEIVE #############
    socket = context.socket(zmq.SUB)
    socket.connect(ip)
    socket.setsockopt_string(zmq.SUBSCRIBE, str(''))


def get_data():
    global context, socket
    message = None
    i = 0
    while i &lt; 1:
        try:
            zmq_response = socket.recv()
            # print(&quot;has received&quot;)
            message = json.loads(zmq_response)
            i = i + 1
        except Exception as e:
            print(&quot;Exception: &quot;, e)
    return message


def send_trigger(trigger: str):
   global socket_pub

   message = {&quot;trigger&quot;: trigger}
   socket_pub.send_string(json.dumps(message))

</code></pre>
<p>This group of functions creates a publisher and a subscriber, based on the ZMQ library and the TCP protocol, used to create a double communication channel between the host pc and the Zynq7000. The functions <code>init_listener</code> and <code>init_sender</code> initialize the subscriber and the publisher respectively. Both take as argument the respective IP address publisher/subscriber (or rather the host PC and the Zynq7000). For the publisher we will adopt the technique of broadcasting the messages to every subscriber listening to a specific port, so no particular IP address would be required to use this function. <code>send_trigger</code> is used to sync the read/write mechanism between the host pc and the Zynq7000. The host PC listens to the Arty, and sends a message every time a trigger is sent from the Zynq7000 using this function.</p>
<h2 id="load-the-photos-to-be-recognized">Load the photos to be recognized</h2>
<pre><code class="language-python">def init_database(folder: str):
    global database_dict_list
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder, filename))
        features = run(img)
        if img is not None:
            database_dict_list.append({&quot;features&quot;: features, &quot;label&quot;: filename})
</code></pre>
<p>This function loads the photos to be recognized and extracts a vocabulary of features and a label out of it. The features will be used in the Face Recognition process, and the label is used to assign a name if a person is recognized in the process.</p>
<h2 id="main-loop">Main Loop</h2>
<pre><code class="language-python">
def main_sl(ip_address: str, camera_index: int):

    init(ip_address)

    model = YuNet(modelPath=&quot;face_detection_yunet_2022mar.onnx&quot;,
                  inputSize=[320, 320],
                  confThreshold=0.9,
                  nmsThreshold=0.3,
                  topK=5000,
                  )
    try:
        cap.release()
    except:
        pass
    cap = cv2.VideoCapture(camera_index)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    pcl_similarity = 2500
    fr_similarity = 700

    aspect_ratio = (320/320, 320/320)

    print(&quot;Start Smart Lock Routine&quot;)

    while True:

        message_recv = get_data()
        _, img = cap.read()

        if img is None:
            print(&quot;I can't read from the camera!&quot;)
            continue

        result_segmenter = ec.euclidean_clustering(message_recv[&quot;point_cloud_camera&quot;])
        res = fe3d.feature_extractor_3d(result_segmenter, message_recv[&quot;point_cloud_database&quot;])

        if res &gt; pcl_similarity:
            print(&quot;NOT A PERSON!&quot;)
            print(res)
            send_trigger(&quot;continue&quot;)
            continue

        sim_ = 99999
        person_ = &quot;&quot;
        try:
            image = img.copy()
            image = cv2.resize(image, (320, 320))
            img = cv2.resize(img, (320, 320))
            model.setInputSize([320, 320])
            result = model.infer(img)
            output = None
            for det in result:
                bbox = det[0:4].astype(np.int32)
                output = bbox
                break

            image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])]

            features = run(image)
            for elem in database_dict_list:
                sim_face = np.linalg.norm(features - elem[&quot;features&quot;])

                if sim_face &lt; fr_similarity:
                    if sim_face &lt; sim_:
                        sim_ = sim_face
                        person_ = elem[&quot;label&quot;]
            if sim_ != 99999:
                sim_ = 99999
                print(&quot;person is: &quot;, person_)
            else:
                print(&quot;UNKOWN&quot;)

            send_trigger(&quot;continue&quot;)
        except:
            send_trigger(&quot;continue&quot;)
            pass

</code></pre>
<p>In the main loop we put together all the functions to create our desired flow. </p>
<p>First the publisher, subscriber and vocabulary of people are initialized through the function <code>init</code>. </p>
<pre><code class="language-python">init(ip_address)
</code></pre>
<p>Secondly, we load the model of Face Detection in the variable <code>model</code>.</p>
<pre><code class="language-python">model = YuNet(modelPath=&quot;face_detection_yunet_2022mar.onnx&quot;,
          inputSize=[320, 320],
          confThreshold=0.9,
          nmsThreshold=0.3,
          topK=5000,
          )
</code></pre>
<p>We initialize the camera using the variable <code>cap</code>. This is our object which grabs the 2D frames.</p>
<pre><code class="language-python">try:
    cap.release()
except:
    pass
cap = cv2.VideoCapture(camera_index)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
</code></pre>
<p>We set our similarity threshold values for Face Recognition (<code>fr_similarity</code>) and 3D Person Detection (<code>pcl_similarity</code>). The lower those values are the more robust the system is, but with less probability to give an output.</p>
<pre><code class="language-python">pcl_similarity = 2500
fr_similarity = 700
</code></pre>
<p>We receive the messages from the publisher of the 3D acquisition and grab simultaneously a frame from the 2D camera:</p>
<pre><code class="language-python">message_recv = get_data()
_, img = cap.read()

if img is None:
    print(&quot;I can't read from the camera!&quot;)
    continue
</code></pre>
<p>We check that there is actually a face in front of the camera.</p>
<pre><code class="language-python">result_segmenter = ec.euclidean_clustering(message_recv[&quot;point_cloud_camera&quot;])
res = fe3d.feature_extractor_3d(result_segmenter, message_recv[&quot;point_cloud_database&quot;])

if res &gt; pcl_similarity:
    print(&quot;NOT A PERSON!&quot;)
    print(res)
    send_trigger(&quot;continue&quot;)
    continue
</code></pre>
<p>If there is a person, we start the 2D branch, by firstly detecting and cropping a face.</p>
<pre><code class="language-python">image = img.copy()
image = cv2.resize(image, (320, 320))
img = cv2.resize(img, (320, 320))
model.setInputSize([320, 320])
result = model.infer(img)
output = None
for det in result:
    bbox = det[0:4].astype(np.int32)
    output = bbox
    break

image = image[int(bbox[1]*aspect_ratio[1]):int((bbox[1]+bbox[3])*aspect_ratio[1]), int(bbox[0]*aspect_ratio[1]):int((bbox[0]+bbox[2])*aspect_ratio[1])]

</code></pre>
<p>We perform Face Recognition:</p>
<pre><code class="language-python">features = run(image)
</code></pre>
<p>And then we check if the person is present into the database, and if so we assign a label to the most likely similar person, otherwise we restart the loop:</p>
<pre><code class="language-python">    for elem in database_dict_list:
        sim_face = np.linalg.norm(features - elem[&quot;features&quot;])

        if sim_face &lt; fr_similarity:
            if sim_face &lt; sim_:
                sim_ = sim_face
                person_ = elem[&quot;label&quot;]
    if sim_ != 99999:
        sim_ = 99999
        print(&quot;person is: &quot;, person_)
    else:
        print(&quot;UNKOWN&quot;)
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../kria-kv260/" class="btn btn-neutral float-left" title="Kria KV260"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../testbench/kv260/" class="btn btn-neutral float-right" title="Kria KV260">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../kria-kv260/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../testbench/kv260/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
