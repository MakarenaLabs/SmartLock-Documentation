<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://smartlock.musebox.it/code-analysis/kria-kv260/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Kria KV260 - SmartLock Design Reference</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Kria KV260";
        var mkdocs_page_input_path = "code-analysis/kria-kv260.md";
        var mkdocs_page_url = "/code-analysis/kria-kv260/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> SmartLock Design Reference
        </a>
        <div class="version">
          v2.0.4
        </div><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Introduction</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../project-description/">Project Description</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../hw-sw-requirements/">Hardware & Software requirements</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Run the examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/how-to-run/">How to run the demo</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/running-on-kria-kv260/">Running on Kria KV260</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/running-on-arty-z7/">Running on Arty Z7-20</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Code analysis</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Kria KV260</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#variables">Variables</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#zmq">ZMQ</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#publisher">Publisher</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#subscriber">Subscriber</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#handshake">Handshake</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3d-camera">3D Camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#libroyale3d-library">LibRoyale3D library</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#constructor">Constructor</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#3d-camera-detection">3D camera detection</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#3d-camera-initialization">3D camera initialization</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#setting-the-3d-camera-options">Setting the 3D camera options</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#starting-to-capture-of-the-data">Starting to capture of the data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#setting-the-exposure-time">Setting the exposure time</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2d-camera">2D camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#initialization">Initialization</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#camera-aperture">Camera aperture</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#processing-branches">Processing branches</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#3d-processing">3D Processing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2d-processing-and-recognition">2D Processing and Recognition</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#musebox-messages-exchange">MuseBox messages exchange</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#face-detection">Face Detection</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#face-recognition">Face Recognition</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#face-landmark">Face Landmark</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../arty-z7/">Arty Z7-20</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Testbench</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../testbench/kv260/">Kria KV260</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">SmartLock Design Reference</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Code analysis</li>
      <li class="breadcrumb-item active">Kria KV260</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="kria-kv260-example">Kria KV260 Example</h1>
<h2 id="introduction">Introduction</h2>
<p>We will now paste the code of the example application of the SmartLock running on a Kria SOM (in our case a KV260 board) and explain how it works piece by piece.</p>
<p>The project is composed of 4 main classes:</p>
<ol>
<li><code>easy_zmq</code>, which implements the communication between the software and MuseBox Server using ZMQ library;</li>
<li><code>easy_ncurses</code>, which is used to manage the information showed in the console window;</li>
<li><code>cv_show</code>, which exposes some functions to draw a graphic interface;</li>
<li><code>camera2d</code>, which handles 2D camera functions;</li>
</ol>
<p>We use <code>ncurses</code> library to have a better output displayed in the terminal window.</p>
<p>Then, we now dig step by step and explain all the software functionalities.</p>
<details style="padding: 10px; background: #bedcf0; border-radius: 5px">
<summary>Click to expand the entire <b>main.cpp</b> content</summary>


<pre><code class="language-c++">// #include &lt;royale.hpp&gt;

#include &quot;../include/utils.hpp&quot;
#include &quot;../include/camera2d.hpp&quot;
#include &quot;../include/cv_show.hpp&quot;
#include &quot;../include/3d_face_points.hpp&quot;
#include &quot;../include/easy_zmq.hpp&quot;
#include &quot;../include/easy_ncurses.hpp&quot;

#include &quot;../include/point_cloud/feature_extractor_3d.hpp&quot;
#include &quot;../include/point_cloud/downsampler.hpp&quot;
#include &quot;../include/point_cloud/segmenter.hpp&quot;

#include &quot;../include/libroyale3d/LibRoyale3D.hpp&quot;

pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr cloud(new pcl::PointCloud&lt;pcl::PointXYZ&gt;());
pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr cloud_person(new pcl::PointCloud&lt;pcl::PointXYZ&gt;());
pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr cloud_clustered(new pcl::PointCloud&lt;pcl::PointXYZ&gt;());

int main(int argc, char *argv[])
{
    if (argc &lt; 3)
    {
        std::cerr &lt;&lt; &quot; need as input the number of the camera and similarity&quot; &lt;&lt; std::endl;
    }

    unsigned int index = std::atoi(argv[1]);
    float similarity_ = std::atof(argv[2]);
    int listening_port = 5556;

    bool unlocked = false;
    int button_pressed = -1;

    bool debug = false;
    bool show = false;
    bool ncur_active = true;
    bool fullscreen = true;

    for (int j = 3; j &lt; argc; ++j)
    {
        if (strcmp(argv[j], &quot;--debug&quot;) == 0)
        {
            debug = true;
            std::cout &lt;&lt; &quot;Debug mode activated.&quot; &lt;&lt; std::endl;
        }

        if (strcmp(argv[j], &quot;--show&quot;) == 0)
        {
            show = true;
            std::cout &lt;&lt; &quot;Show mode activated.&quot; &lt;&lt; std::endl;
        }

        if (strcmp(argv[j], &quot;--port&quot;) == 0)
        {
            listening_port = std::atoi(argv[++j]);
            std::cout &lt;&lt; &quot;Subscriber will listen to port &quot; &lt;&lt; listening_port &lt;&lt; &quot;.&quot; &lt;&lt; std::endl;
        }

        if (strcmp(argv[j], &quot;--no-ncur&quot;) == 0)
        {
            ncur_active = false;
            std::cout &lt;&lt; &quot;Removed ncurses&quot; &lt;&lt; std::endl;
        }

        if (strcmp(argv[j], &quot;--no-fullscreen&quot;) == 0)
        {
            fullscreen = false;
            std::cout &lt;&lt; &quot;Removed fullscreen mode&quot; &lt;&lt; std::endl;
        }
    }

    std::string version = PROJECT_VER;

    /*************************************** ZMQ ***************************************/

    std::string listening_url = &quot;tcp://127.0.0.1:&quot; + std::to_string(listening_port);
    std::string publishing_url = &quot;tcp://127.0.0.1:9696&quot;;

    easy_zmq zmq(27000);
    void *publisher = zmq.create_publisher();
    void *subscriber = zmq.create_subscriber();

    zmq.bind_publisher(publisher, publishing_url);
    zmq.connect_subscriber(subscriber, listening_url, &quot;&quot;);

    zmq.handshake(publisher);

    /*************************************** 3D part with new library ***************************************/

    LibRoyale3D libroyale = LibRoyale3D();
    std::cout &lt;&lt; LOG_3D_CAMERA(&quot;Camera connected&quot;) &lt;&lt; std::endl;

    segmenter *ec = new segmenter();
    feature_extractor_3d *fe = new feature_extractor_3d(metrics::L2);

    for (unsigned int i = 0; i &lt; face_points.size(); ++i)
    {
        pcl::PointXYZ point;
        point.x = face_points.at(i).at(0);
        point.y = face_points.at(i).at(1);
        point.z = face_points.at(i).at(2);
        cloud_person-&gt;push_back(point);
    }

    /*************************************** 2D part ***************************************/

    camera2d camera2d(index);
    camera2d.open();
    bool camera2d_detected = camera2d.detected();

    /*************************************** Init ***************************************/

    easy_ncurses ncur(version, debug, ncur_active);
    ncur.write_init();

    std::map&lt;std::string, int&gt; bounding_box;
    cv_show cv_show(&quot;SmartLock Routine - v&quot; + version, fullscreen);
    cv::Mat image;
    int mb_status;

    int OK_THRESHOLD = 3;
    int ok_count = OK_THRESHOLD;
    std::string previousPerson = &quot;&quot;;

    while (camera2d_detected &amp;&amp; libroyale.DETECTED) {
        try{
            if(show &amp;&amp; !cv_show.draw_image(image, mb_status)){
                break;
            }

            ncur.set_face_detection(FACE_DETECTED == 1);
            ncur.set_face_recognition(FACE_DETECTED == 1, FACE_RECOGNIZED);

            if(unlocked){
                button_pressed = -1;
                while(button_pressed != 114){

                    button_pressed = cv::waitKey(1);

                    if(button_pressed == 27){
                        zmq.close_connection(publisher);
                        zmq.close_connection(subscriber);
                        return 0;
                    }

                    continue;
                }
                unlocked = false;
            }

            camera2d_detected = camera2d.detected();
            ncur.set_cameras_status(camera2d_detected, libroyale.DETECTED);

            image = camera2d.capture();
            cv_show.draw_circles(image, 0);

            ncur.clear_lines(19, 1);
            ncur.clear_lines(14, 1);

            if (!libroyale.LOCKED) {
                libroyale.lock();

                *cloud = libroyale.get_camera_point_cloud();

                if (libroyale.has_captured() &amp;&amp; cloud-&gt;points.size() &gt; 1) {
                    *cloud_clustered = ec-&gt;clustering(cloud);

                    std::tuple&lt;float, pcl::PointCloud&lt;pcl::VFHSignature308&gt;&gt; features = fe-&gt;get_features(cloud_clustered, cloud_person);
                    float similarity = std::get&lt;0&gt;(features);

                    pcl::PointCloud&lt;pcl::PointXYZ&gt;::iterator begin_pcl = cloud-&gt;begin();
                    pcl::PointCloud&lt;pcl::PointXYZ&gt;::iterator end_pcl = cloud-&gt;end();

                    cloud-&gt;erase(begin_pcl, end_pcl);

                    if(debug){
                        ncur.write_similarity(similarity_, similarity);
                    }

                    if(similarity &gt;= similarity_){
                        FACE_DETECTED = 0;
                        ok_count = OK_THRESHOLD;
                        libroyale.unlock();
                        continue;
                    }
                } else {
                    FACE_DETECTED = 0;
                    ok_count = OK_THRESHOLD;
                    libroyale.unlock();
                    continue;
                }

                libroyale.unlock();

            } else {
                FACE_DETECTED = 0;
                ok_count = OK_THRESHOLD;
                libroyale.unlock();
                continue;
            }

            FACE_DETECTED = 1;
            FACE_RECOGNIZED = &quot;0&quot;;
            cv_show.draw_circles(image, 0);

            // face detection task
            nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image);
            mb_status = zmq.send_message(publisher, fd_message.dump());

            nlohmann::json fd_response = zmq.receive_message(subscriber);

            // check if there is only 1 face
            if (json_key_exists(fd_response, &quot;data&quot;)){

                if(show) {
                    cv_show.draw_bounding_boxes(image, fd_response[&quot;data&quot;]);
                }

                if(fd_response[&quot;data&quot;].size() == 0){
                    FACE_DETECTED = 0;
                    ok_count = OK_THRESHOLD;
                } else if (fd_response[&quot;data&quot;].size() == 1) {

                    ncur.clear_lines(19, 1);

                    int x = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;x&quot;].get&lt;int&gt;();
                    int y = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;y&quot;].get&lt;int&gt;();
                    int width = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;width&quot;].get&lt;int&gt;();
                    int height = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;height&quot;].get&lt;int&gt;();

                    bounding_box = cv_show.set_bounding_box(x, y, width, height);

                    std::string personFound = &quot;&quot;;
                    std::vector&lt;float&gt; landmarks = {0};

                    cv::Mat cropped_face;

                    if(bounding_box[&quot;width&quot;] &lt; cv_show::BB_LIMIT){
                        FACE_RECOGNIZED = &quot;0&quot;;
                        ok_count = OK_THRESHOLD;
                        cv_show.draw_circles(image, 2);
                    } else {
                        // face recognition
                        cropped_face = crop_face(image, bounding_box);
                        if (!cropped_face.empty()) {
                            nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face);
                            mb_status = zmq.send_message(publisher, fr_message.dump());

                            nlohmann::json fr_response = zmq.receive_message(subscriber);
                            personFound = fr_response[&quot;data&quot;][&quot;prediction&quot;] == &quot;unknown&quot; ? &quot;&quot; : fr_response[&quot;data&quot;][&quot;prediction&quot;];

                            if(strcmp(personFound.c_str(), &quot;&quot;) == 0){
                                // face not recognized
                                // face landmark
                                FACE_RECOGNIZED = &quot;0&quot;;
                                ok_count = OK_THRESHOLD;
                                cv_show.draw_circles(image, 2);

                                nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face);
                                mb_status = zmq.send_message(publisher, fl_message.dump());

                                nlohmann::json fl_response = zmq.receive_message(subscriber);

                                landmarks = fl_response[&quot;data&quot;][&quot;prediction&quot;].get&lt;std::vector&lt;float&gt;&gt;();
                                std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug));

                                int ld_line = 14;
                                ncur.clear_lines(ld_line, 4);
                                ncur.write_line(ld_log, ld_line);

                            } else {
                                // face recognized

                                if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){
                                    if(ok_count-- &lt;= 0){
                                        FACE_RECOGNIZED = personFound;
                                        cv_show.draw_thumbnail(image, personFound);
                                        cv_show.draw_circles(image, 1);
                                        cv_show.draw_name(image, personFound, bounding_box);
                                        cv_show.door_unlocked(image);
                                        unlocked = true;
                                    }
                                } else {
                                    ok_count = OK_THRESHOLD;
                                }
                            }
                        }
                    }

                    previousPerson = personFound;

                } else {
                    // here there are more than 1 face
                    FACE_RECOGNIZED = &quot;0&quot;;
                    ok_count = OK_THRESHOLD;
                    ncur.write_line(&quot;There must be only 1 face&quot;, 19);
                }
            }

        } catch(std::exception &amp;e){
            FACE_DETECTED = 0;
            FACE_RECOGNIZED = &quot;0&quot;;
            ok_count = OK_THRESHOLD;
            libroyale.unlock();
            ncur.write_init();
            continue;
        }
    }

    // destroy
    zmq.close_connection(publisher);
    zmq.close_connection(subscriber);

    return 0;
}
</code></pre>


</details>
<p><br></p>
<h2 id="variables">Variables</h2>
<p>Some of the most important variables used into the code are:</p>
<ul>
<li><code>index</code>: 2D camera index;</li>
<li><code>similarity_</code>: it sets the 3D threshold to accept something in front of the camera as a face;</li>
<li><code>min_epsilon</code>: it is the similarity threshold for the Face Recognition task; </li>
<li><code>listening_port</code>: it is the port where the software listens to the MuseBox Server responses;</li>
</ul>
<h2 id="zmq">ZMQ</h2>
<p>First of all, we must set the communication between the software and the MuseBox Server using our <code>easy_zmq</code> class.</p>
<p>The <code>zmq</code> object receives as input the buffer length, which is set to <code>27000</code> by default. </p>
<pre><code class="language-c++">std::string listening_url = &quot;tcp://127.0.0.1:&quot; + std::to_string(listening_port);
std::string publishing_url = &quot;tcp://127.0.0.1:9696&quot;;

easy_zmq zmq(27000);
void *publisher = zmq.create_publisher();
void *subscriber = zmq.create_subscriber();

zmq.bind_publisher(publisher, publishing_url);
zmq.connect_subscriber(subscriber, listening_url, &quot;&quot;);

zmq.handshake(publisher);
</code></pre>
<h3 id="publisher">Publisher</h3>
<p>The creation of the publisher is entrusted to the appropriate function:</p>
<pre><code class="language-c++">void* easy_zmq::create_publisher(){
    void* publisher = zmq_socket(context, ZMQ_PUB);
    assert(publisher);
    return publisher;
}
</code></pre>
<p>The connection to the listening url is created with the following function:</p>
<pre><code class="language-c++">void easy_zmq::bind_publisher(void* publisher, std::string publishing_url){
    int bind = zmq_bind(publisher, publishing_url.c_str());
    if (bind &lt; 0) {
        unsigned int error_code = zmq_errno();
        std::cout &lt;&lt; &quot;zmq_bind server ctx error: &quot; &lt;&lt; error_code &lt;&lt; &quot;, &quot; &lt;&lt; zmq_strerror(error_code) &lt;&lt; &quot;\n&quot; &lt;&lt; std::endl;
        assert(bind == 0);
    }
}
</code></pre>
<h3 id="subscriber">Subscriber</h3>
<p>The creation of the subscriber is entrusted to the appropriate function:</p>
<pre><code class="language-c++">void* easy_zmq::create_subscriber() {
    void* subscriber = zmq_socket(context, ZMQ_SUB);
    assert(subscriber);
    return subscriber;
}
</code></pre>
<p>The subscriber connection is made by the following function:</p>
<pre><code class="language-c++">void easy_zmq::connect_subscriber(void* subscriber, std::string listening_url, std::string topic){
    int connect = zmq_connect(subscriber, listening_url.c_str());
    if(connect &lt; 0){
        std::cout &lt;&lt; &quot;Subscriber not connected&quot; &lt;&lt; std::endl;
    }
    int rc = zmq_setsockopt(subscriber, ZMQ_SUBSCRIBE, reinterpret_cast&lt;void*&gt;(&amp;topic), 0);
    int timeout = 500;
    zmq_setsockopt(subscriber, ZMQ_RCVTIMEO, &amp;timeout, sizeof(timeout));
}
</code></pre>
<h3 id="handshake">Handshake</h3>
<p>The handshake phase is necessary to initialize the communication with MuseBox Server. It is made by:</p>
<pre><code class="language-c++">void easy_zmq::handshake(void* publisher) {
    nlohmann::json message = {{&quot;topic&quot;, &quot;Handshake&quot;}};
    int out = easy_zmq::send_message(publisher, message.dump());
    sleep(1);
    std::cout &lt;&lt; &quot;Handshake done...&quot; &lt;&lt; std::endl;
}
</code></pre>
<p>that it simply sends a message with the topic set to <code>Handshake</code>.</p>
<h2 id="3d-camera">3D Camera</h2>
<p>The following code refers to the 3D ToF camera. Please take a look also to <a href="../../project-description/#3d-apis">3D APIs</a> section for more information about the 3D APIs used by these pieces of code.</p>
<p>We have used a custom library named <code>LibRoyale3D</code> to easily manage the APIs. In our main file the initialization of the 3D camera is made with the code below. In fact, to initialize and make all the needed checks about the 3D camera, we must easily initialize our library with the following call:</p>
<pre><code class="language-c++">LibRoyale3D libroyale = LibRoyale3D();
std::cout &lt;&lt; LOG_3D_CAMERA(&quot;Camera connected&quot;) &lt;&lt; std::endl;
</code></pre>
<p>Once the <code>libroyale</code> variable is defined, we are aware about the 3D camera connection and initialization. Let's deeply analize the content of this library. Most of the functions return a status code which indicates the status of the connection.</p>
<h3 id="libroyale3d-library">LibRoyale3D library</h3>
<p>In this chapter we are going to explain all the functions exposed by our <code>LibRoyale3D</code> library.</p>
<h4 id="constructor">Constructor</h4>
<p>First of all, the constructor of our library is able to make some checks and initializations, such as the 3D camera detection, its initialization, the setting of some filters, etc.:</p>
<pre><code class="language-c++">LibRoyale3D::LibRoyale3D() {

    status st = detect();
    DETECTED = st == status::STATUS_OK;

    if(!DETECTED){
        return;
    }

    st = initialize();
    INITD = st == status::STATUS_OK;

    if(!INITD){
        return;
    }

    retrieve_streams();
    set_filters();

    camera-&gt;registerDataListener(&amp;depth_data_listener);

    if(DETECTED &amp;&amp; INITD){
        start_capture();
    }

    LOCKED = depth_data_listener.guard;
    INFO = std::string(&quot;LibRoyale3D - v&quot;) + std::string(LIBROYALE3D_PROJECT_VER) + std::string(&quot; - MakarenaLabs SRL&quot;);
}
</code></pre>
<h4 id="3d-camera-detection">3D camera detection</h4>
<p>The first action of the construction is the detection of the 3D camera. It is made by the following function:</p>
<pre><code class="language-c++">status LibRoyale3D::detect(){

    royale::CameraManager manager;
    royale::Vector&lt;royale::String&gt; camera_list;

    camera_list = manager.getConnectedCameraList();

    if (camera_list.empty()) {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;No 3D camera detected&quot; &lt;&lt; std::endl;
        return status::NO_CAMERA_DETECTED;
    }

    camera = manager.createCamera(camera_list[0]); // we have only one camera connected!!!

    if (camera == nullptr) {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Can not create the 3D camera&quot; &lt;&lt; std::endl;
        return status::NO_CAMERA_CREATED;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;3D camera detected&quot; &lt;&lt; std::endl;
    return status::STATUS_OK;
}
</code></pre>
<h4 id="3d-camera-initialization">3D camera initialization</h4>
<p>Then, we initialize the camera:</p>
<pre><code class="language-c++">status LibRoyale3D::initialize(){
    // Before the camera device is ready we have to invoke initialize on it.
    if (camera-&gt;initialize() != royale::CameraStatus::SUCCESS)
    {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Cannot initialize the 3D camera&quot; &lt;&lt; std::endl;
        return status::NO_CAMERA_INITIALIZED;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;3D camera initialized&quot; &lt;&lt; std::endl;
    return status::STATUS_OK;
}
</code></pre>
<h3 id="setting-the-3d-camera-options">Setting the 3D camera options</h3>
<p>After the camera initialization, we set the density of the depth information in input:</p>
<pre><code class="language-c++">status LibRoyale3D::retrieve_streams(){
    if (camera-&gt;getStreams(streamIds) != royale::CameraStatus::SUCCESS)
    {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Cannot retrieve streams from the 3D camera&quot; &lt;&lt; std::endl;
        return status::NO_CAMERA_STREAMS;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;Retrieving streams from the 3D camera&quot; &lt;&lt; std::endl;
    return status::STATUS_OK;
}

status LibRoyale3D::set_filters(){
#if LIBROYALE_VERSION == 4
    if (camera-&gt;setFilterLevel(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS)
#else
    if (camera-&gt;setFilterPreset(royale::FilterPreset::Binning_8_Basic, streamIds[0]) != royale::CameraStatus::SUCCESS)
#endif
    {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Cannot set filter for stream &quot; &lt;&lt; std::to_string(streamIds[0]) &lt;&lt; std::endl;
        return status::NO_CAMERA_FILTERS;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;Set filter for stream &quot; &lt;&lt; std::to_string(streamIds[0]) &lt;&lt; std::endl;
    return status::STATUS_OK;
}
</code></pre>
<h4 id="starting-to-capture-of-the-data">Starting to capture of the data</h4>
<p>If all the previous steps end without any problems, we start to obtain the 3D data. To do this, we attach a callback function which will convert the depth information in a Point Cloud and return it for our 3D processing branch:</p>
<pre><code class="language-c++">status LibRoyale3D::start_capture(){
    // start capture mode
    if (camera-&gt;startCapture() != royale::CameraStatus::SUCCESS)
    {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Error starting the capturing&quot; &lt;&lt; std::endl;
        return status::NO_CAMERA_CAPTURE;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;Ready to capture data&quot; &lt;&lt; std::endl;
    return status::STATUS_OK;
}
</code></pre>
<h4 id="setting-the-exposure-time">Setting the exposure time</h4>
<p>Finally, we set the exposure time of the camera:</p>
<pre><code class="language-c++">status LibRoyale3D::set_exposure_time(int exposure){
    if (camera-&gt;setExposureTime(exposure, streamIds[0]) != royale::CameraStatus::SUCCESS)
    {
        std::cout &lt;&lt; &quot;[LIBROYALE3D][ERROR] &quot; &lt;&lt; &quot;Cannot set exposure time for stream &quot; &lt;&lt; std::to_string(streamIds[0]) &lt;&lt; std::endl;
        return status::NO_CAMERA_EXPOSURE_TIME;
    }

    std::cout &lt;&lt; &quot;[LIBROYALE3D][INFO] &quot; &lt;&lt; &quot;Changed exposure time for stream &quot; &lt;&lt; std::to_string(streamIds[0]) &lt;&lt; &quot; to &quot; &lt;&lt; exposure &lt;&lt; &quot; microseconds&quot; &lt;&lt; std::endl;
    exposure_time = exposure;
    return status::STATUS_OK;
}
</code></pre>
<p>If all the previous tasks run correctly, we have successfully connected the 3D camera:</p>
<pre><code class="language-c++">std::cout &lt;&lt; LOG_3D_CAMERA(&quot;Camera connected&quot;) &lt;&lt; std::endl;
</code></pre>
<h2 id="2d-camera">2D camera</h2>
<p>Now, we are ready to initialize the 2D camera using our class, passing as input its index:</p>
<pre><code class="language-c++">camera2d camera2d(index);
camera2d.open();
bool camera2d_detected = camera2d.detected();
</code></pre>
<h3 id="initialization">Initialization</h3>
<p>Previously, we initialize the 2D camera object passing as input its index:</p>
<pre><code class="language-c++">camera2d::camera2d(int _index) {
    index = _index;
}
</code></pre>
<h3 id="camera-aperture">Camera aperture</h3>
<p>Once we have opened the camera, we can start capturing the frames. This piece of code uses OpenCV to init the camera in the <code>cap</code> class global variable and checks whether it is connected or not.
Then, it assigns <code>True</code> to the <code>DETECTED</code> class global variable if the camera is ready and opened, <code>False</code> otherwise.</p>
<pre><code class="language-c++">void camera2d::open(){

    if(index &lt; 0){
        index = get_index(15);
    }

    cap.open(index);

    if (!cap.isOpened()) {
        std::cerr &lt;&lt; LOG_2D_CAMERA(&quot;Cannot open camera&quot;) &lt;&lt; std::endl;
        DETECTED = false;
    } else {
        cap.set(cv::CAP_PROP_BUFFERSIZE, 1);
        try {
            cv::Mat test_image;
            cap.read(test_image);
            DETECTED = true;
            std::cout &lt;&lt; LOG_2D_CAMERA(&quot;Camera connected&quot;) &lt;&lt; std::endl;
        } catch (...) {
            std::cerr &lt;&lt; LOG_2D_CAMERA(&quot;Cannot open camera&quot;) &lt;&lt; std::endl;
            DETECTED = false;
        }
    }
}
</code></pre>
<h2 id="processing-branches">Processing branches</h2>
<p>The software processing flow is divided into two parts:</p>
<ol>
<li><a href="#3d-processing">3D processing branch</a></li>
<li><a href="#2d-processing-and-recognition">2D processing and recognition branch</a></li>
</ol>
<p>Face Detection, Face Recognition and Face Landmark AI tasks are included inside the 2D processing phase. </p>
<h3 id="3d-processing">3D Processing</h3>
<p>The first procedure which our SmartLock loop executes is the 3D processing branch.
This branch receives the Point Cloud from the callback of the 3D camera.</p>
<p>The first operation is to segment the face out of the Point Cloud received with the segmenter object:</p>
<pre><code class="language-c++">*cloud_clustered = ec-&gt;clustering(cloud);
</code></pre>
<p>The second operation is to extract the features and the similarity of what is in front of the camera to a 3D face</p>
<pre><code class="language-c++">std::tuple&lt;float, pcl::PointCloud&lt;pcl::VFHSignature308&gt;&gt; features = fe-&gt;get_features(cloud_clustered, cloud_person);
float similarity = std::get&lt;0&gt;(features);
</code></pre>
<p>If the Point Clouds are similar, in front of the camera there is an actual face, and we can proceed, otherwise we restart the loop</p>
<pre><code class="language-c++">if(similarity &gt;= similarity_){
    FACE_DETECTED = 0;
    ok_count = OK_THRESHOLD;
    libroyale.unlock();
    continue;
}
</code></pre>
<h3 id="2d-processing-and-recognition">2D Processing and Recognition</h3>
<p>The last step, if an actual 3D face is found in front of the camera, is to recognize it. This part of the code does the recognition task, which is composed by:</p>
<ol>
<li>Face Detection;</li>
<li>Face Recognition;</li>
<li>Face Landmark (to suggest the user to stay straight in front of the camera);</li>
</ol>
<p>Before the 3D processing phase (we did this before to have a continuity in the display of images with OpenCV), we have captured the camera frame using the <code>camera2d</code> class:</p>
<pre><code class="language-c++">image = camera2d.capture();
</code></pre>
<p>where the function is:</p>
<pre><code class="language-c++">cv::Mat camera2d::capture(){
    // grab 2D camera frame
    cv::Mat local_image;
    cap.read(local_image);
    cv::resize(local_image, local_image, cv::Size(IMG_WIDTH, IMG_HEIGHT));

    cv::putText(local_image, &quot;Press 'Esc' to close SmartLock Routine&quot;, cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, BLACK, 3);
    cv::putText(local_image, &quot;Press 'Esc' to close SmartLock Routine&quot;, cv::Point(10, IMG_HEIGHT-10), cv::FONT_HERSHEY_PLAIN, 1.2, WHITE, 1.5);

    return local_image;
}
</code></pre>
<p>which returns our frame (resized to be compatible with the AI models) as <code>cv::Mat</code> object.</p>
<p>With this frame, we can proceed with the entire recognition flow, which is:</p>
<div style="width: 100%;">
    <img src="../../images/recog_flow.png" style="display: block; margin-left: auto; margin-right: auto; width: 70%;" alt="SmartLock Design Reference Architecture">
    <p style="text-align: center; font-style: italic">Fig. 1 - SmartLock Recognition Flow</p>
</div>

<h4 id="musebox-messages-exchange">MuseBox messages exchange</h4>
<p>For all the following task, MuseBox needs the same message's structure.</p>
<pre><code class="language-c++">nlohmann::json create_mb_message(std::string topic, std::string publishing_url, std::string listening_url, cv::Mat image, double min_epsilon)
{
    // create the vector that &quot;flats&quot; the cv::Mat in a single dimension
    std::vector&lt;uchar&gt; img_buf;
    cv::imencode(&quot;.jpg&quot;, image, img_buf);
    auto *enc_msg = reinterpret_cast&lt;unsigned char *&gt;(img_buf.data());
    std::string encoded = base64_encode(enc_msg, img_buf.size());

    nlohmann::json message = {
        {&quot;clientId&quot;, &quot;1&quot;}, // client ID
        {&quot;topic&quot;, topic},  // Machine Learning task
        {&quot;publisherMusebox&quot;, publishing_url},
        {&quot;publisherQueue&quot;, listening_url}, // where the MuseBox subscriber will respond
        {&quot;image&quot;, encoded},                // image to infer
    };

    if (topic != topic_name[topics::FACE_DETECTION])
    {
        message[&quot;only_face&quot;] = true;
    }
    if (topic == topic_name[topics::FACE_RECOGNITION])
    {
        message[&quot;min_epsilon&quot;] = min_epsilon; // face recognition threshold
    }

    return message;
}
</code></pre>
<p>In this function, first of all, we need to compress our image before sending it to the server. We do that with:</p>
<pre><code class="language-c++">// create the vector that &quot;flats&quot; the cv::Mat in a single dimension
std::vector&lt;uchar&gt; img_buf;
cv::imencode(&quot;.jpg&quot;, image, img_buf);
auto *enc_msg = reinterpret_cast&lt;unsigned char *&gt;(img_buf.data());
std::string encoded = base64_encode(enc_msg, img_buf.size());
</code></pre>
<p>Then, we are able to prepare the JSON request to be sent:</p>
<pre><code class="language-c++">nlohmann::json message = {
    {&quot;clientId&quot;, &quot;1&quot;}, // client ID
    {&quot;topic&quot;, topic},  // Machine Learning task
    {&quot;publisherMusebox&quot;, publishing_url},
    {&quot;publisherQueue&quot;, listening_url}, // where the MuseBox subscriber will respond
    {&quot;image&quot;, encoded},                // image to infer
};
</code></pre>
<p>For Face Recognition and Face Landmark tasks, we have to send the <code>only_face</code> flag set to <code>True</code>.
Only for Face Recognition task, we have to send the <code>min_epsilon</code> threshold.</p>
<pre><code class="language-c++">if (topic != topic_name[topics::FACE_DETECTION])
{
    message[&quot;only_face&quot;] = true;
}
if (topic == topic_name[topics::FACE_RECOGNITION])
{
    message[&quot;min_epsilon&quot;] = min_epsilon; // face recognition threshold
}
</code></pre>
<p>Now we are ready to send and receive the messages to/from MuseBox Server. To do that, we use two functions:</p>
<pre><code class="language-c++">int easy_zmq::send_message(void* publisher, std::string message){
    int output = zmq_send(publisher, message.c_str(), message.size(), 0);
    if (output &lt; 0) {
        unsigned int error_code = zmq_errno();
        std::cout &lt;&lt; &quot;server ctx error: &quot; &lt;&lt; error_code &lt;&lt; &quot;, &quot; &lt;&lt; zmq_strerror(error_code) &lt;&lt; &quot;\n&quot; &lt;&lt; std::endl;
    }
    return output;
}

nlohmann::json easy_zmq::receive_message(void* subscriber){
    // blocking receive for ZMQ
    int nbytes = zmq_recv(subscriber, buffer, buffer_length, 0);
    std::string str_msg = buffer;
    if(str_msg == &quot;&quot; || str_msg.size() == 0 || nbytes == -1){
        str_msg = &quot;{}&quot;;
    }
    nlohmann::json message = nlohmann::json::parse(str_msg);
    std::fill_n(buffer, buffer_length, 0);
    return message;
}
</code></pre>
<h4 id="face-detection">Face Detection</h4>
<p>The Face Detection phase starts with the creation of the message to send to the MuseBox Server:</p>
<pre><code class="language-c++">// face detection task
nlohmann::json fd_message = create_mb_message(topic_name[topics::FACE_DETECTION], publishing_url, listening_url, image, 0);
mb_status = zmq.send_message(publisher, fd_message.dump());

ncur.clear_lines(19, 1);

nlohmann::json fd_response = zmq.receive_message(subscriber);
</code></pre>
<p>once a face is detected (only 1 face is permitted):</p>
<pre><code class="language-c++">if (fd_response[&quot;data&quot;].size() == 1) {

    ncur.clear_lines(19, 1);

    int x = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;x&quot;].get&lt;int&gt;();
    int y = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;y&quot;].get&lt;int&gt;();
    int width = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;width&quot;].get&lt;int&gt;();
    int height = fd_response[&quot;data&quot;][0][&quot;boundingBox&quot;][&quot;height&quot;].get&lt;int&gt;();

    bounding_box = cv_show.set_bounding_box(x, y, width, height);

    std::string personFound = &quot;&quot;;
    std::vector&lt;float&gt; landmarks = { 0 };

    cv::Mat cropped_face;

    [...]

}
</code></pre>
<p>we save the bounding box's dimensions received from MuseBox Server, and we can go ahead with the next phase.</p>
<h4 id="face-recognition">Face Recognition</h4>
<p>The next step is to send the image of the face detected to the MuseBox Face Recognition task. To do this, we must crop the image following the bounding box's dimensions:</p>
<pre><code class="language-c++">cropped_face = crop_face(image, bounding_box);
</code></pre>
<p>where the <code>crop_face</code> function is:</p>
<pre><code class="language-c++">cv::Mat crop_face(cv::Mat image, std::map&lt;std::string, int&gt; bounding_box)
{
    cv::Mat cropped_face;
    cv::Rect roi;
    roi.x = bounding_box[&quot;x&quot;];
    roi.y = bounding_box[&quot;y&quot;];
    roi.width = bounding_box[&quot;width&quot;];
    roi.height = bounding_box[&quot;height&quot;];

    try
    {
        cropped_face = image(roi);
    }
    catch (cv::Exception &amp;e)
    {
        std::cout &lt;&lt; &quot;Couldn't crop image.&quot; &lt;&lt; std::endl;
    }
    return cropped_face;
}
</code></pre>
<p>Then, we can set and send the Face Recognition message to the MuseBox Server. If the person is recognized, its name is saved into <code>personFound</code> variable.
If the MuseBox Server returns <code>unknown</code> value, <code>personFound</code> variable is set to empty string.</p>
<pre><code class="language-c++">nlohmann::json fr_message = create_mb_message(topic_name[topics::FACE_RECOGNITION], publishing_url, listening_url, cropped_face, min_epsilon);
mb_status = zmq.send_message(publisher, fr_message.dump());

nlohmann::json fr_response = zmq.receive_message(subscriber);
personFound = fr_response[&quot;personFound&quot;] == &quot;unknown&quot; ? &quot;&quot; : fr_response[&quot;personFound&quot;];
</code></pre>
<p>Here two scenarios open up:</p>
<ol>
<li>Face is recognized;</li>
<li>Face is not recognized;</li>
</ol>
<p>The detected face is recognized when <code>personFound</code> variable is not empty. In this case, we want to be sure that the recognized person in this turn is the same of the last one.
A person is recognized when it is the same for 3 consecutive times (<code>int OK_THRESHOLD = 3;</code>).</p>
<pre><code class="language-c++">// face recognized

if(std::strcmp(previousPerson.c_str(), personFound.c_str()) == 0){
    if(ok_count-- &lt;= 0){
        FACE_RECOGNIZED = personFound;
        cv_show.draw_thumbnail(image, personFound);
        cv_show.draw_circles(image, 1);
        cv_show.draw_name(image, personFound, bounding_box);
    }
} else {
    ok_count = OK_THRESHOLD;
}
</code></pre>
<p>Otherwise, if the person is not recognized, it could be they are not facing straight to the camera. To suggest them to do so, we use Face Landmark task.</p>
<h4 id="face-landmark">Face Landmark</h4>
<p>First of all, we must send the message to the MuseBox Server, and we must wait the response:</p>
<pre><code class="language-c++">// face not recognized
// face landmark
FACE_RECOGNIZED = &quot;0&quot;;
ok_count = OK_THRESHOLD;
cv_show.draw_circles(image, 2);

nlohmann::json fl_message = create_mb_message(topic_name[topics::FACE_LANDMARK], publishing_url, listening_url, cropped_face, 0);
mb_status = zmq.send_message(publisher, fl_message.dump());

nlohmann::json fl_response = zmq.receive_message(subscriber);
</code></pre>
<p>The MuseBox Server returns the array of landmark points we can use whether to suggest or not to the user to face straight in front of the camera.</p>
<pre><code class="language-c++">landmarks = fl_response[&quot;landmarks&quot;].get&lt;std::vector&lt;float&gt;&gt;();
std::string ld_log(cv_show.draw_landmarks(image, bounding_box, landmarks, debug));
</code></pre>
<p>In the <code>draw_landmarks</code> function, we use three landmark points:</p>
<ol>
<li>Nose</li>
<li>Left ear</li>
<li>Right ear</li>
</ol>
<p>We check the pan and the roll of the person's face. If the distance between the three points is out of the threshold, we show the suggestion.</p>
<pre><code class="language-c++">std::string cv_show::draw_landmarks(cv::Mat image, std::map&lt;std::string, int&gt; bounding_box, std::vector&lt;float&gt; landmarks, bool debug)
{
    std::string to_return = &quot;&quot;;
    std::vector&lt;int&gt; points = {57, 0, 32};

    int nose = 57;
    int ear_sx = 0;
    int ear_dx = 32;

    // panning
    float dx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_dx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_dx*2+1], 2));
    float sx_nose_distance = std::sqrt(std::pow(landmarks[nose*2] - landmarks[ear_sx*2], 2) + std::pow(landmarks[nose*2+1] - landmarks[ear_sx*2+1], 2));
    float pan_threshold = 30;

    // rolling
    float ears_distance = std::abs(landmarks[ear_dx*2+1] - landmarks[ear_sx*2+1]);
    float roll_threshold = 30;

    if(std::abs(dx_nose_distance - sx_nose_distance) &gt; pan_threshold || ears_distance &gt; roll_threshold){
        std::string _msg = &quot;Please, keep your face straight&quot;;
        cv::putText(image, _msg, cv::Point(bounding_box[&quot;x&quot;]+10, bounding_box[&quot;y&quot;]+bounding_box[&quot;height&quot;]-10), cv::FONT_HERSHEY_PLAIN, 1, BLACK, 3);
        cv::putText(image, _msg, cv::Point(bounding_box[&quot;x&quot;]+10, bounding_box[&quot;y&quot;]+bounding_box[&quot;height&quot;]-10), cv::FONT_HERSHEY_PLAIN, 1, WHITE, 1.5);

        to_return = &quot;[WARN] - &quot; + _msg;
    }

    [...]

}
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../examples/running-on-arty-z7/" class="btn btn-neutral float-left" title="Running on Arty Z7-20"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../arty-z7/" class="btn btn-neutral float-right" title="Arty Z7-20">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../examples/running-on-arty-z7/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../arty-z7/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
